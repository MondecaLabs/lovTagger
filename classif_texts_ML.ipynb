{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mondeca/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import string\n",
    "import re\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_validate, cross_val_predict\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier    \n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, cohen_kappa_score, classification_report , precision_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open('DATA.pkl', 'rb') as handle:\n",
    "    X, Y, vocabs = pickle.load(handle)\n",
    "    \n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The event's size in regard to its thematic area and/or locality. .  the event's sponsor . The rate given by the users. An App Concept is nominated for the award. Jury's rate. links to the award that this App Concept won. organizer. short title. url to the registration. The event edition. . Users' rate. Submission. won award. apps4X is an RDF vocabulary defined to facilitate the description of Open-Data-based Co-creation events. a link to the registration for the event. sponsor. one or more awards offered at this co-creation event. A jury consists of . Jury. edition. the event's organizer. The actual prize offered. A co-creation event is a competition, a contest, a challenge etc. It's a subproperty as it is an event but with a certain scope. An App Concept idea as it was submitted to a certain competition. prize. The rate given by the jury. The App Concept can be within a certain theme. consists of. Award. the award offered by the jury to a certain App Concept. theme. nominated for. award offered. A submission consists of . apps4X is an RDF vocabulary defined to facilitate the description of Open-Data-based Co-creation events. the event's short title and/or its abbreviation. the size of the event. award's ceremnoy. Co-creation event. The award ceremony. Jury Member. A jury evaluating the submissions to a co-creation event. \""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "656 656 656\n"
     ]
    }
   ],
   "source": [
    "print(len(X), len(Y), len(vocabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "totals = Counter(i for i in list(itertools.chain.from_iterable(Y)))\n",
    "distrib = dict(totals)\n",
    "#distrib = sorted(distrib.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags_len = np.sum(list(distrib.values()))\n",
    "distrib_freq = {key:val/all_tags_len for key,val in distrib.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(distrib.keys(), distrib.values())\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.s\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the number of uppercase in string\n",
    "def num_uppercase(string):\n",
    "    count = 0\n",
    "    for l in string:\n",
    "        if l.isupper():\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# From a list of words, split the words with more than one uppercase\n",
    "def split_uppercases(words):\n",
    "    words_proc = []\n",
    "    for word in words:\n",
    "        if num_uppercase(word) >= 1 and not(word.isupper()):\n",
    "            splitted_words = re.findall('[A-Za-z][a-z]*', word)\n",
    "            for w in splitted_words:\n",
    "                words_proc.append(w.lower())\n",
    "        else:\n",
    "            words_proc.append(word.lower())\n",
    "    return words_proc\n",
    "\n",
    "# Tokenizer\n",
    "def Tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "# Tokenizer : use stems of words\n",
    "def Tokenizer_lemm(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    porter_stemmer=nltk.PorterStemmer()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "# new Tokenizer : split the strings in the form \"jointAnnual\" with a space in the text\n",
    "def newTokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).split()\n",
    "    words = split_uppercases(words)\n",
    "    #porter_stemmer=nltk.PorterStemmer()\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    #words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('description', 'description'),\n",
       " ('knowledgegrouping', 'knowledge'),\n",
       " ('organization', 'grouping'),\n",
       " ('division', 'organization'),\n",
       " ('organizationalunit', 'division'),\n",
       " ('teaches', 'organizational'),\n",
       " ('institution', 'unit'),\n",
       " ('html', 'teaches'),\n",
       " ('subject', 'institution'),\n",
       " ('knowledgegrouping', 'html')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare both tokenizer\n",
    "[(x,y) for x,y in zip(Tokenizer(X[10]), newTokenizer(X[10]))][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.25, max_features=None, min_df=0.0025,\n",
       "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function Tokenizer at 0x7f9a68439400>, vocabulary=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Study the CountVectorizer\n",
    "CV = CountVectorizer(tokenizer=Tokenizer, min_df=.0025, max_df=0.25, ngram_range=(1,3))\n",
    "CV.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(CV.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline : Vectorizer + tfidf + SVD\n",
    "pipeline1 = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(tokenizer=Tokenizer_lemm, min_df=.0025, max_df=0.25, ngram_range=(1,3))),\n",
    "            ('svd', TruncatedSVD(algorithm='randomized', n_components=100, random_state=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of classifiers which work with multilabels\n",
    "clfs = {\n",
    "'RF': RandomForestClassifier(n_estimators=100, random_state=1, n_jobs=50),\n",
    "'MLP': MLPClassifier(solver='lbfgs',alpha=1e-5,hidden_layer_sizes=(100),random_state=1),\n",
    "'KPPV': KNeighborsClassifier(n_neighbors=7)\n",
    "}\n",
    "\n",
    "# Fonction de test des diffÃ©rents classifieurs\n",
    "def run_classifiers(clfs,X,Y, pipeline):\n",
    "    # Apply processing pipeline\n",
    "    X_proc = pipeline.fit_transform(X)\n",
    "    # Cross Validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1_macro', 'precision_micro', 'recall_micro']\n",
    "    for i in clfs:\n",
    "        try:\n",
    "            clf = clfs[i]\n",
    "            print(\"\\n\\n======= {0} =======\".format(i))\n",
    "            y_pred = cross_val_predict(clf, X_proc, Y, cv=kf)\n",
    "            print(classification_report(Y, y_pred))            \n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform multilablels Y in sparse matrix for sklearn\n",
    "mlb = MultiLabelBinarizer(sparse_output=False)\n",
    "Y_mlb = mlb.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the tags with a low frequency inferior of seuil\n",
    "# Some graphs will then not have any tags anymore, these graphs are the removed\n",
    "def remove_low_frequencies(X, Y_mlb, vocabs, seuil = 0.02):\n",
    "    labels_freqs = np.sum(Y_mlb, axis = 0) / np.sum(Y_mlb)\n",
    "    # Keeps only labels with freq > seuil\n",
    "    inds_col = labels_freqs > seuil\n",
    "    Y_filt = Y_mlb * inds_col\n",
    "    \n",
    "    inds_0_col = np.where((Y_filt == 0).all(axis=0))[0]\n",
    "    mask0 = np.ones(Y_filt.shape[1], np.bool)\n",
    "    mask0[inds_0_col] = 0\n",
    "    Y_filt_col = Y_filt[:,mask0]\n",
    "    Y_filt_col.shape\n",
    "    \n",
    "    inds_0 = np.where((Y_filt_col == 0).all(axis=1))[0] # Indices of rows fully equals 0\n",
    "    mask = np.ones(len(Y_filt_col), np.bool)\n",
    "    mask[inds_0] = 0\n",
    "    Y_filt_rows = Y_filt_col[mask,:]\n",
    "    \n",
    "    vocabs_filt = vocabs[mask]\n",
    "    X_filt = X[mask]\n",
    "    Y_filt = Y_filt_rows\n",
    "    \n",
    "    return X_filt, Y_filt, vocabs_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filt, Y_filt, vocabs_filt = remove_low_frequencies(X,Y_mlb, vocabs, seuil = 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(424,) (424, 15) (424,)\n"
     ]
    }
   ],
   "source": [
    "print(X_filt.shape, Y_filt.shape, vocabs_filt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======= RF =======\n",
      "[Errno 12] Cannot allocate memory\n",
      "\n",
      "\n",
      "======= MLP =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.52      0.44        31\n",
      "           1       0.27      0.28      0.27        36\n",
      "           2       0.20      0.29      0.24        28\n",
      "           3       0.47      0.60      0.53        40\n",
      "           4       0.41      0.42      0.42        33\n",
      "           5       0.48      0.46      0.47        28\n",
      "           6       0.26      0.30      0.28        47\n",
      "           7       0.43      0.40      0.42        50\n",
      "           8       0.30      0.36      0.33        28\n",
      "           9       0.33      0.32      0.33        31\n",
      "          10       0.21      0.22      0.22        27\n",
      "          11       0.21      0.30      0.25        33\n",
      "          12       0.17      0.21      0.19        33\n",
      "          13       0.18      0.22      0.20        36\n",
      "          14       0.19      0.28      0.23        25\n",
      "\n",
      "   micro avg       0.30      0.35      0.32       506\n",
      "   macro avg       0.30      0.35      0.32       506\n",
      "weighted avg       0.31      0.35      0.33       506\n",
      " samples avg       0.28      0.37      0.30       506\n",
      "\n",
      "\n",
      "\n",
      "======= KPPV =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        31\n",
      "           1       0.60      0.08      0.15        36\n",
      "           2       0.46      0.21      0.29        28\n",
      "           3       0.33      0.03      0.05        40\n",
      "           4       0.75      0.09      0.16        33\n",
      "           5       0.56      0.18      0.27        28\n",
      "           6       0.50      0.06      0.11        47\n",
      "           7       0.50      0.06      0.11        50\n",
      "           8       1.00      0.04      0.07        28\n",
      "           9       1.00      0.03      0.06        31\n",
      "          10       0.25      0.04      0.06        27\n",
      "          11       0.00      0.00      0.00        33\n",
      "          12       0.50      0.03      0.06        33\n",
      "          13       1.00      0.11      0.20        36\n",
      "          14       0.50      0.04      0.07        25\n",
      "\n",
      "   micro avg       0.52      0.07      0.12       506\n",
      "   macro avg       0.53      0.07      0.11       506\n",
      "weighted avg       0.53      0.07      0.11       506\n",
      " samples avg       0.07      0.06      0.06       506\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# With only the most frequent tags\n",
    "run_classifiers(clfs, X_filt, Y_filt, pipeline1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======= RF =======\n",
      "[Errno 12] Cannot allocate memory\n",
      "\n",
      "\n",
      "======= MLP =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.26      0.25        31\n",
      "           1       0.31      0.31      0.31        13\n",
      "           2       0.47      0.64      0.55        14\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.25      0.17      0.20        36\n",
      "           5       0.15      0.22      0.18         9\n",
      "           6       0.31      0.29      0.30        28\n",
      "           7       0.12      0.12      0.12        16\n",
      "           8       0.55      0.43      0.48        14\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       1.00      0.83      0.91         6\n",
      "          11       0.10      0.11      0.10        19\n",
      "          12       0.40      0.47      0.44        40\n",
      "          13       0.50      0.47      0.49        19\n",
      "          14       0.19      0.24      0.21        17\n",
      "          15       0.43      0.38      0.40         8\n",
      "          16       0.50      0.17      0.25         6\n",
      "          17       0.34      0.33      0.34        33\n",
      "          18       0.47      0.50      0.48        28\n",
      "          19       0.17      0.19      0.18        47\n",
      "          20       0.43      0.48      0.45        50\n",
      "          21       0.33      0.25      0.29        16\n",
      "          22       0.12      0.20      0.15         5\n",
      "          23       0.58      0.64      0.61        11\n",
      "          24       0.00      0.00      0.00         2\n",
      "          25       0.48      0.46      0.47        28\n",
      "          26       0.17      0.12      0.14         8\n",
      "          27       0.26      0.23      0.24        31\n",
      "          28       0.13      0.11      0.12        27\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       0.44      0.33      0.38        12\n",
      "          31       0.20      0.20      0.20         5\n",
      "          32       0.62      0.56      0.59         9\n",
      "          33       0.16      0.21      0.18        33\n",
      "          34       0.13      0.15      0.14        33\n",
      "          35       0.19      0.25      0.21        36\n",
      "          36       0.71      0.67      0.69        15\n",
      "          37       0.44      0.37      0.40        19\n",
      "          38       0.80      0.67      0.73         6\n",
      "          39       0.40      0.36      0.38        11\n",
      "          40       0.27      0.32      0.29        25\n",
      "          41       0.17      0.17      0.17        12\n",
      "          42       0.44      0.36      0.40        11\n",
      "\n",
      "   micro avg       0.31      0.32      0.31       798\n",
      "   macro avg       0.33      0.31      0.31       798\n",
      "weighted avg       0.32      0.32      0.31       798\n",
      " samples avg       0.27      0.33      0.28       798\n",
      "\n",
      "\n",
      "\n",
      "======= KPPV =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        31\n",
      "           1       0.00      0.00      0.00        13\n",
      "           2       0.00      0.00      0.00        14\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.00      0.00      0.00        36\n",
      "           5       0.00      0.00      0.00         9\n",
      "           6       0.56      0.18      0.27        28\n",
      "           7       0.00      0.00      0.00        16\n",
      "           8       0.75      0.21      0.33        14\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       1.00      1.00      1.00         6\n",
      "          11       0.00      0.00      0.00        19\n",
      "          12       1.00      0.03      0.05        40\n",
      "          13       1.00      0.26      0.42        19\n",
      "          14       0.00      0.00      0.00        17\n",
      "          15       0.00      0.00      0.00         8\n",
      "          16       0.00      0.00      0.00         6\n",
      "          17       0.25      0.03      0.05        33\n",
      "          18       0.83      0.18      0.29        28\n",
      "          19       0.50      0.04      0.08        47\n",
      "          20       0.75      0.06      0.11        50\n",
      "          21       0.00      0.00      0.00        16\n",
      "          22       0.00      0.00      0.00         5\n",
      "          23       1.00      0.09      0.17        11\n",
      "          24       0.00      0.00      0.00         2\n",
      "          25       0.50      0.04      0.07        28\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00        31\n",
      "          28       0.00      0.00      0.00        27\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       1.00      0.17      0.29        12\n",
      "          31       0.00      0.00      0.00         5\n",
      "          32       1.00      0.33      0.50         9\n",
      "          33       0.00      0.00      0.00        33\n",
      "          34       0.00      0.00      0.00        33\n",
      "          35       0.75      0.08      0.15        36\n",
      "          36       1.00      0.47      0.64        15\n",
      "          37       0.00      0.00      0.00        19\n",
      "          38       0.00      0.00      0.00         6\n",
      "          39       1.00      0.18      0.31        11\n",
      "          40       0.75      0.12      0.21        25\n",
      "          41       0.00      0.00      0.00        12\n",
      "          42       0.80      0.36      0.50        11\n",
      "\n",
      "   micro avg       0.63      0.07      0.13       798\n",
      "   macro avg       0.34      0.09      0.13       798\n",
      "weighted avg       0.39      0.07      0.11       798\n",
      " samples avg       0.08      0.08      0.08       798\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# all tags, WITH LEMMATIZER \n",
    "run_classifiers(clfs, X, Y_mlb, pipeline1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======= RF =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        31\n",
      "           1       0.00      0.00      0.00        13\n",
      "           2       0.00      0.00      0.00        14\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.00      0.00      0.00        36\n",
      "           5       0.00      0.00      0.00         9\n",
      "           6       0.00      0.00      0.00        28\n",
      "           7       0.00      0.00      0.00        16\n",
      "           8       1.00      0.07      0.13        14\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       1.00      0.33      0.50         6\n",
      "          11       0.00      0.00      0.00        19\n",
      "          12       1.00      0.10      0.18        40\n",
      "          13       1.00      0.05      0.10        19\n",
      "          14       0.00      0.00      0.00        17\n",
      "          15       0.00      0.00      0.00         8\n",
      "          16       0.00      0.00      0.00         6\n",
      "          17       0.00      0.00      0.00        33\n",
      "          18       0.80      0.14      0.24        28\n",
      "          19       0.00      0.00      0.00        47\n",
      "          20       0.00      0.00      0.00        50\n",
      "          21       0.00      0.00      0.00        16\n",
      "          22       0.00      0.00      0.00         5\n",
      "          23       1.00      0.27      0.43        11\n",
      "          24       0.00      0.00      0.00         2\n",
      "          25       0.00      0.00      0.00        28\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00        31\n",
      "          28       0.00      0.00      0.00        27\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00         5\n",
      "          32       0.00      0.00      0.00         9\n",
      "          33       0.00      0.00      0.00        33\n",
      "          34       0.00      0.00      0.00        33\n",
      "          35       0.00      0.00      0.00        36\n",
      "          36       1.00      0.33      0.50        15\n",
      "          37       0.00      0.00      0.00        19\n",
      "          38       1.00      0.17      0.29         6\n",
      "          39       1.00      0.09      0.17        11\n",
      "          40       0.00      0.00      0.00        25\n",
      "          41       0.00      0.00      0.00        12\n",
      "          42       0.00      0.00      0.00        11\n",
      "\n",
      "   micro avg       0.61      0.03      0.05       798\n",
      "   macro avg       0.20      0.04      0.06       798\n",
      "weighted avg       0.18      0.03      0.05       798\n",
      " samples avg       0.03      0.03      0.03       798\n",
      "\n",
      "\n",
      "\n",
      "======= MLP =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.35      0.35        31\n",
      "           1       0.18      0.23      0.20        13\n",
      "           2       0.48      0.71      0.57        14\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.16      0.19      0.18        36\n",
      "           5       0.33      0.22      0.27         9\n",
      "           6       0.30      0.29      0.29        28\n",
      "           7       0.20      0.19      0.19        16\n",
      "           8       0.80      0.57      0.67        14\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       1.00      0.83      0.91         6\n",
      "          11       0.06      0.05      0.05        19\n",
      "          12       0.57      0.65      0.60        40\n",
      "          13       0.45      0.47      0.46        19\n",
      "          14       0.15      0.24      0.18        17\n",
      "          15       0.25      0.25      0.25         8\n",
      "          16       0.14      0.17      0.15         6\n",
      "          17       0.32      0.36      0.34        33\n",
      "          18       0.50      0.43      0.46        28\n",
      "          19       0.21      0.28      0.24        47\n",
      "          20       0.45      0.40      0.43        50\n",
      "          21       0.31      0.25      0.28        16\n",
      "          22       0.00      0.00      0.00         5\n",
      "          23       0.55      0.55      0.55        11\n",
      "          24       0.00      0.00      0.00         2\n",
      "          25       0.47      0.50      0.48        28\n",
      "          26       0.10      0.12      0.11         8\n",
      "          27       0.27      0.26      0.26        31\n",
      "          28       0.25      0.22      0.24        27\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       0.57      0.33      0.42        12\n",
      "          31       0.00      0.00      0.00         5\n",
      "          32       0.67      0.67      0.67         9\n",
      "          33       0.18      0.21      0.20        33\n",
      "          34       0.16      0.18      0.17        33\n",
      "          35       0.23      0.28      0.25        36\n",
      "          36       0.64      0.60      0.62        15\n",
      "          37       0.41      0.47      0.44        19\n",
      "          38       0.75      0.50      0.60         6\n",
      "          39       0.56      0.45      0.50        11\n",
      "          40       0.21      0.24      0.22        25\n",
      "          41       0.00      0.00      0.00        12\n",
      "          42       0.38      0.27      0.32        11\n",
      "\n",
      "   micro avg       0.32      0.33      0.32       798\n",
      "   macro avg       0.32      0.30      0.31       798\n",
      "weighted avg       0.33      0.33      0.33       798\n",
      " samples avg       0.28      0.35      0.30       798\n",
      "\n",
      "\n",
      "\n",
      "======= KPPV =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        31\n",
      "           1       0.00      0.00      0.00        13\n",
      "           2       0.50      0.07      0.12        14\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.00      0.00      0.00        36\n",
      "           5       0.00      0.00      0.00         9\n",
      "           6       0.50      0.07      0.12        28\n",
      "           7       0.00      0.00      0.00        16\n",
      "           8       0.83      0.36      0.50        14\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       1.00      0.83      0.91         6\n",
      "          11       0.00      0.00      0.00        19\n",
      "          12       0.33      0.03      0.05        40\n",
      "          13       0.83      0.26      0.40        19\n",
      "          14       0.00      0.00      0.00        17\n",
      "          15       0.00      0.00      0.00         8\n",
      "          16       0.00      0.00      0.00         6\n",
      "          17       0.40      0.06      0.11        33\n",
      "          18       1.00      0.21      0.35        28\n",
      "          19       0.00      0.00      0.00        47\n",
      "          20       0.67      0.04      0.08        50\n",
      "          21       0.00      0.00      0.00        16\n",
      "          22       0.00      0.00      0.00         5\n",
      "          23       0.33      0.09      0.14        11\n",
      "          24       0.00      0.00      0.00         2\n",
      "          25       1.00      0.04      0.07        28\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00        31\n",
      "          28       0.00      0.00      0.00        27\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       1.00      0.17      0.29        12\n",
      "          31       0.00      0.00      0.00         5\n",
      "          32       1.00      0.33      0.50         9\n",
      "          33       0.60      0.18      0.28        33\n",
      "          34       0.00      0.00      0.00        33\n",
      "          35       1.00      0.06      0.11        36\n",
      "          36       1.00      0.40      0.57        15\n",
      "          37       0.00      0.00      0.00        19\n",
      "          38       0.00      0.00      0.00         6\n",
      "          39       1.00      0.18      0.31        11\n",
      "          40       1.00      0.08      0.15        25\n",
      "          41       0.00      0.00      0.00        12\n",
      "          42       1.00      0.27      0.43        11\n",
      "\n",
      "   micro avg       0.66      0.07      0.13       798\n",
      "   macro avg       0.35      0.09      0.13       798\n",
      "weighted avg       0.39      0.07      0.11       798\n",
      " samples avg       0.09      0.07      0.08       798\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing pipeline : No lemm\n",
    "pipeline_nolemm = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, min_df=.0025, max_df=0.25, ngram_range=(1,3))),\n",
    "            ('svd', TruncatedSVD(algorithm='randomized', n_components=100, random_state=1))])\n",
    "\n",
    "# WITHOUT LEMMATIZER\n",
    "run_classifiers(clfs, X, Y_mlb, pipeline_nolemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW TOKENIZER (ngram_range=1 lower the score of 1%)\n",
    "pipeline2 = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(tokenizer=newTokenizer, min_df=.0025, max_df=0.25, ngram_range=(1,3))),\n",
    "            ('svd', TruncatedSVD(algorithm='randomized', n_components=150, random_state=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======= RF =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        31\n",
      "           1       0.00      0.00      0.00        13\n",
      "           2       0.00      0.00      0.00        14\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.00      0.00      0.00        36\n",
      "           5       0.00      0.00      0.00         9\n",
      "           6       0.00      0.00      0.00        28\n",
      "           7       0.00      0.00      0.00        16\n",
      "           8       1.00      0.14      0.25        14\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       1.00      0.50      0.67         6\n",
      "          11       0.00      0.00      0.00        19\n",
      "          12       1.00      0.10      0.18        40\n",
      "          13       0.00      0.00      0.00        19\n",
      "          14       0.00      0.00      0.00        17\n",
      "          15       0.00      0.00      0.00         8\n",
      "          16       0.00      0.00      0.00         6\n",
      "          17       0.00      0.00      0.00        33\n",
      "          18       1.00      0.14      0.25        28\n",
      "          19       0.00      0.00      0.00        47\n",
      "          20       0.33      0.02      0.04        50\n",
      "          21       0.00      0.00      0.00        16\n",
      "          22       0.00      0.00      0.00         5\n",
      "          23       1.00      0.36      0.53        11\n",
      "          24       0.00      0.00      0.00         2\n",
      "          25       1.00      0.04      0.07        28\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00        31\n",
      "          28       0.00      0.00      0.00        27\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00         5\n",
      "          32       0.00      0.00      0.00         9\n",
      "          33       0.00      0.00      0.00        33\n",
      "          34       0.50      0.03      0.06        33\n",
      "          35       0.00      0.00      0.00        36\n",
      "          36       1.00      0.47      0.64        15\n",
      "          37       0.00      0.00      0.00        19\n",
      "          38       1.00      0.17      0.29         6\n",
      "          39       0.00      0.00      0.00        11\n",
      "          40       0.00      0.00      0.00        25\n",
      "          41       0.00      0.00      0.00        12\n",
      "          42       0.00      0.00      0.00        11\n",
      "\n",
      "   micro avg       0.70      0.04      0.07       798\n",
      "   macro avg       0.21      0.05      0.07       798\n",
      "weighted avg       0.23      0.04      0.06       798\n",
      " samples avg       0.04      0.04      0.04       798\n",
      "\n",
      "\n",
      "\n",
      "======= MLP =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.45      0.41        31\n",
      "           1       0.33      0.31      0.32        13\n",
      "           2       0.56      0.36      0.43        14\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.24      0.28      0.26        36\n",
      "           5       0.40      0.22      0.29         9\n",
      "           6       0.23      0.29      0.25        28\n",
      "           7       0.11      0.12      0.11        16\n",
      "           8       0.56      0.64      0.60        14\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       1.00      0.83      0.91         6\n",
      "          11       0.33      0.16      0.21        19\n",
      "          12       0.48      0.50      0.49        40\n",
      "          13       0.56      0.47      0.51        19\n",
      "          14       0.18      0.24      0.21        17\n",
      "          15       0.43      0.38      0.40         8\n",
      "          16       0.50      0.17      0.25         6\n",
      "          17       0.40      0.36      0.38        33\n",
      "          18       0.52      0.46      0.49        28\n",
      "          19       0.24      0.23      0.24        47\n",
      "          20       0.44      0.46      0.45        50\n",
      "          21       0.36      0.31      0.33        16\n",
      "          22       0.25      0.20      0.22         5\n",
      "          23       0.86      0.55      0.67        11\n",
      "          24       0.00      0.00      0.00         2\n",
      "          25       0.52      0.46      0.49        28\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.35      0.26      0.30        31\n",
      "          28       0.11      0.07      0.09        27\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       0.42      0.42      0.42        12\n",
      "          31       0.40      0.40      0.40         5\n",
      "          32       0.62      0.56      0.59         9\n",
      "          33       0.22      0.24      0.23        33\n",
      "          34       0.15      0.18      0.16        33\n",
      "          35       0.25      0.28      0.26        36\n",
      "          36       0.64      0.60      0.62        15\n",
      "          37       0.40      0.42      0.41        19\n",
      "          38       0.75      0.50      0.60         6\n",
      "          39       0.71      0.45      0.56        11\n",
      "          40       0.21      0.24      0.23        25\n",
      "          41       0.00      0.00      0.00        12\n",
      "          42       0.44      0.36      0.40        11\n",
      "\n",
      "   micro avg       0.34      0.33      0.34       798\n",
      "   macro avg       0.36      0.31      0.33       798\n",
      "weighted avg       0.35      0.33      0.34       798\n",
      " samples avg       0.30      0.35      0.31       798\n",
      "\n",
      "\n",
      "\n",
      "======= KPPV =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        31\n",
      "           1       0.00      0.00      0.00        13\n",
      "           2       0.00      0.00      0.00        14\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       1.00      0.03      0.05        36\n",
      "           5       0.00      0.00      0.00         9\n",
      "           6       0.50      0.21      0.30        28\n",
      "           7       0.00      0.00      0.00        16\n",
      "           8       1.00      0.21      0.35        14\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       1.00      0.83      0.91         6\n",
      "          11       0.00      0.00      0.00        19\n",
      "          12       0.00      0.00      0.00        40\n",
      "          13       1.00      0.21      0.35        19\n",
      "          14       0.00      0.00      0.00        17\n",
      "          15       0.00      0.00      0.00         8\n",
      "          16       0.00      0.00      0.00         6\n",
      "          17       0.25      0.03      0.05        33\n",
      "          18       1.00      0.18      0.30        28\n",
      "          19       0.00      0.00      0.00        47\n",
      "          20       0.75      0.06      0.11        50\n",
      "          21       0.00      0.00      0.00        16\n",
      "          22       0.00      0.00      0.00         5\n",
      "          23       0.50      0.09      0.15        11\n",
      "          24       0.00      0.00      0.00         2\n",
      "          25       0.00      0.00      0.00        28\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00        31\n",
      "          28       0.00      0.00      0.00        27\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       1.00      0.25      0.40        12\n",
      "          31       0.00      0.00      0.00         5\n",
      "          32       1.00      0.11      0.20         9\n",
      "          33       0.62      0.15      0.24        33\n",
      "          34       0.00      0.00      0.00        33\n",
      "          35       0.50      0.03      0.05        36\n",
      "          36       1.00      0.33      0.50        15\n",
      "          37       0.50      0.05      0.10        19\n",
      "          38       0.00      0.00      0.00         6\n",
      "          39       0.00      0.00      0.00        11\n",
      "          40       1.00      0.08      0.15        25\n",
      "          41       0.00      0.00      0.00        12\n",
      "          42       0.75      0.27      0.40        11\n",
      "\n",
      "   micro avg       0.65      0.06      0.11       798\n",
      "   macro avg       0.31      0.07      0.11       798\n",
      "weighted avg       0.36      0.06      0.10       798\n",
      " samples avg       0.07      0.06      0.07       798\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# New tokenizer (split maj words)\n",
    "run_classifiers(clfs, X, Y_mlb, pipeline2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======= RF =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 24 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        31\n",
      "           1       0.00      0.00      0.00        13\n",
      "           2       0.00      0.00      0.00        14\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.00      0.00      0.00        36\n",
      "           5       0.00      0.00      0.00         9\n",
      "           6       0.00      0.00      0.00        28\n",
      "           7       0.00      0.00      0.00        16\n",
      "           8       0.50      0.07      0.12        14\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       1.00      0.33      0.50         6\n",
      "          11       0.00      0.00      0.00        19\n",
      "          12       0.83      0.12      0.22        40\n",
      "          13       1.00      0.05      0.10        19\n",
      "          14       0.00      0.00      0.00        17\n",
      "          15       1.00      0.12      0.22         8\n",
      "          16       0.00      0.00      0.00         6\n",
      "          17       0.00      0.00      0.00        33\n",
      "          18       1.00      0.18      0.30        28\n",
      "          19       0.00      0.00      0.00        47\n",
      "          20       0.33      0.02      0.04        50\n",
      "          21       0.00      0.00      0.00        16\n",
      "          22       0.00      0.00      0.00         5\n",
      "          23       1.00      0.36      0.53        11\n",
      "          24       0.00      0.00      0.00         2\n",
      "          25       1.00      0.07      0.13        28\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.50      0.03      0.06        31\n",
      "          28       0.00      0.00      0.00        27\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00         5\n",
      "          32       0.00      0.00      0.00         9\n",
      "          33       0.00      0.00      0.00        33\n",
      "          34       0.67      0.06      0.11        33\n",
      "          35       0.00      0.00      0.00        36\n",
      "          36       1.00      0.40      0.57        15\n",
      "          37       0.00      0.00      0.00        19\n",
      "          38       1.00      0.17      0.29         6\n",
      "          39       0.00      0.00      0.00        11\n",
      "          40       0.50      0.04      0.07        25\n",
      "          41       0.00      0.00      0.00        12\n",
      "          42       1.00      0.09      0.17        11\n",
      "\n",
      "   micro avg       0.64      0.04      0.08       798\n",
      "   macro avg       0.29      0.05      0.08       798\n",
      "weighted avg       0.30      0.04      0.07       798\n",
      " samples avg       0.04      0.04      0.04       798\n",
      "\n",
      "\n",
      "\n",
      "======= BAGGING =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 24 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.10      0.15        31\n",
      "           1       0.00      0.00      0.00        13\n",
      "           2       0.50      0.07      0.12        14\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.14      0.03      0.05        36\n",
      "           5       0.00      0.00      0.00         9\n",
      "           6       0.20      0.07      0.11        28\n",
      "           7       0.00      0.00      0.00        16\n",
      "           8       0.77      0.71      0.74        14\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       1.00      0.83      0.91         6\n",
      "          11       0.20      0.05      0.08        19\n",
      "          12       0.50      0.17      0.26        40\n",
      "          13       0.50      0.16      0.24        19\n",
      "          14       0.09      0.06      0.07        17\n",
      "          15       0.50      0.12      0.20         8\n",
      "          16       0.00      0.00      0.00         6\n",
      "          17       0.00      0.00      0.00        33\n",
      "          18       0.43      0.32      0.37        28\n",
      "          19       0.19      0.06      0.10        47\n",
      "          20       0.38      0.10      0.16        50\n",
      "          21       0.50      0.12      0.20        16\n",
      "          22       0.00      0.00      0.00         5\n",
      "          23       1.00      0.36      0.53        11\n",
      "          24       0.00      0.00      0.00         2\n",
      "          25       0.71      0.18      0.29        28\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.36      0.13      0.19        31\n",
      "          28       0.50      0.04      0.07        27\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       0.67      0.17      0.27        12\n",
      "          31       0.00      0.00      0.00         5\n",
      "          32       0.00      0.00      0.00         9\n",
      "          33       0.20      0.06      0.09        33\n",
      "          34       0.31      0.12      0.17        33\n",
      "          35       0.40      0.06      0.10        36\n",
      "          36       0.85      0.73      0.79        15\n",
      "          37       0.57      0.21      0.31        19\n",
      "          38       0.29      0.33      0.31         6\n",
      "          39       0.67      0.18      0.29        11\n",
      "          40       0.25      0.04      0.07        25\n",
      "          41       0.00      0.00      0.00        12\n",
      "          42       0.50      0.18      0.27        11\n",
      "\n",
      "   micro avg       0.38      0.13      0.19       798\n",
      "   macro avg       0.31      0.13      0.17       798\n",
      "weighted avg       0.34      0.13      0.17       798\n",
      " samples avg       0.13      0.12      0.12       798\n",
      "\n",
      "\n",
      "\n",
      "======= ADABOOST =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 24 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.29      0.40        31\n",
      "           1       0.33      0.08      0.12        13\n",
      "           2       0.75      0.43      0.55        14\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.17      0.06      0.08        36\n",
      "           5       0.00      0.00      0.00         9\n",
      "           6       0.08      0.04      0.05        28\n",
      "           7       0.67      0.12      0.21        16\n",
      "           8       0.58      0.50      0.54        14\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       1.00      0.83      0.91         6\n",
      "          11       0.25      0.05      0.09        19\n",
      "          12       0.50      0.28      0.35        40\n",
      "          13       0.57      0.42      0.48        19\n",
      "          14       0.18      0.12      0.14        17\n",
      "          15       1.00      0.12      0.22         8\n",
      "          16       0.50      0.17      0.25         6\n",
      "          17       0.19      0.09      0.12        33\n",
      "          18       0.55      0.39      0.46        28\n",
      "          19       0.23      0.13      0.16        47\n",
      "          20       0.58      0.22      0.32        50\n",
      "          21       0.00      0.00      0.00        16\n",
      "          22       0.00      0.00      0.00         5\n",
      "          23       1.00      0.55      0.71        11\n",
      "          24       0.00      0.00      0.00         2\n",
      "          25       0.36      0.18      0.24        28\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.36      0.16      0.22        31\n",
      "          28       0.14      0.04      0.06        27\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       0.80      0.33      0.47        12\n",
      "          31       0.50      0.20      0.29         5\n",
      "          32       1.00      0.33      0.50         9\n",
      "          33       0.29      0.12      0.17        33\n",
      "          34       0.33      0.12      0.18        33\n",
      "          35       0.20      0.11      0.14        36\n",
      "          36       1.00      0.47      0.64        15\n",
      "          37       0.60      0.16      0.25        19\n",
      "          38       0.50      0.33      0.40         6\n",
      "          39       0.67      0.36      0.47        11\n",
      "          40       0.55      0.24      0.33        25\n",
      "          41       0.00      0.00      0.00        12\n",
      "          42       0.75      0.27      0.40        11\n",
      "\n",
      "   micro avg       0.43      0.19      0.26       798\n",
      "   macro avg       0.41      0.19      0.25       798\n",
      "weighted avg       0.40      0.19      0.25       798\n",
      " samples avg       0.20      0.20      0.20       798\n",
      "\n",
      "\n",
      "\n",
      "======= SVC =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 24 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        31\n",
      "           1       0.00      0.00      0.00        13\n",
      "           2       0.00      0.00      0.00        14\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.00      0.00      0.00        36\n",
      "           5       0.00      0.00      0.00         9\n",
      "           6       0.00      0.00      0.00        28\n",
      "           7       0.00      0.00      0.00        16\n",
      "           8       0.00      0.00      0.00        14\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       0.00      0.00      0.00         6\n",
      "          11       0.00      0.00      0.00        19\n",
      "          12       0.00      0.00      0.00        40\n",
      "          13       0.00      0.00      0.00        19\n",
      "          14       0.00      0.00      0.00        17\n",
      "          15       0.00      0.00      0.00         8\n",
      "          16       0.00      0.00      0.00         6\n",
      "          17       0.00      0.00      0.00        33\n",
      "          18       0.00      0.00      0.00        28\n",
      "          19       0.00      0.00      0.00        47\n",
      "          20       0.00      0.00      0.00        50\n",
      "          21       0.00      0.00      0.00        16\n",
      "          22       0.00      0.00      0.00         5\n",
      "          23       0.00      0.00      0.00        11\n",
      "          24       0.00      0.00      0.00         2\n",
      "          25       0.00      0.00      0.00        28\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.00      0.00      0.00        31\n",
      "          28       0.00      0.00      0.00        27\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00         5\n",
      "          32       0.00      0.00      0.00         9\n",
      "          33       0.00      0.00      0.00        33\n",
      "          34       0.00      0.00      0.00        33\n",
      "          35       0.00      0.00      0.00        36\n",
      "          36       0.00      0.00      0.00        15\n",
      "          37       0.00      0.00      0.00        19\n",
      "          38       0.00      0.00      0.00         6\n",
      "          39       0.00      0.00      0.00        11\n",
      "          40       0.00      0.00      0.00        25\n",
      "          41       0.00      0.00      0.00        12\n",
      "          42       0.00      0.00      0.00        11\n",
      "\n",
      "   micro avg       0.00      0.00      0.00       798\n",
      "   macro avg       0.00      0.00      0.00       798\n",
      "weighted avg       0.00      0.00      0.00       798\n",
      " samples avg       0.00      0.00      0.00       798\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# MULTILABEL with other classifiers with ONEVSREST\n",
    "clf_OvsR = {\n",
    "'RF': OneVsRestClassifier(RandomForestClassifier(n_estimators=100, random_state=1, n_jobs=-1)),\n",
    "'BAGGING': OneVsRestClassifier(BaggingClassifier(n_estimators=100,random_state=1)),\n",
    "'ADABOOST': OneVsRestClassifier(AdaBoostClassifier(n_estimators=100, random_state=1)),\n",
    "'SVC': OneVsRestClassifier(SVC(gamma='scale', decision_function_shape='ovo'))\n",
    "}\n",
    "\n",
    "run_classifiers(clf_OvsR, X, Y_mlb, pipeline2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======= SVC =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 24 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.39      0.48        31\n",
      "           1       0.36      0.31      0.33        13\n",
      "           2       0.62      0.57      0.59        14\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.29      0.25      0.27        36\n",
      "           5       0.75      0.33      0.46         9\n",
      "           6       0.31      0.36      0.33        28\n",
      "           7       0.13      0.12      0.13        16\n",
      "           8       0.54      0.50      0.52        14\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       1.00      1.00      1.00         6\n",
      "          11       0.33      0.16      0.21        19\n",
      "          12       0.47      0.47      0.48        40\n",
      "          13       0.53      0.42      0.47        19\n",
      "          14       0.30      0.41      0.35        17\n",
      "          15       0.33      0.25      0.29         8\n",
      "          16       0.50      0.17      0.25         6\n",
      "          17       0.48      0.33      0.39        33\n",
      "          18       0.42      0.39      0.41        28\n",
      "          19       0.23      0.21      0.22        47\n",
      "          20       0.46      0.32      0.38        50\n",
      "          21       0.38      0.31      0.34        16\n",
      "          22       0.50      0.40      0.44         5\n",
      "          23       0.88      0.64      0.74        11\n",
      "          24       0.00      0.00      0.00         2\n",
      "          25       0.48      0.39      0.43        28\n",
      "          26       0.14      0.12      0.13         8\n",
      "          27       0.48      0.32      0.38        31\n",
      "          28       0.13      0.07      0.10        27\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       0.60      0.50      0.55        12\n",
      "          31       0.50      0.40      0.44         5\n",
      "          32       0.67      0.44      0.53         9\n",
      "          33       0.35      0.42      0.38        33\n",
      "          34       0.12      0.15      0.14        33\n",
      "          35       0.39      0.25      0.31        36\n",
      "          36       0.75      0.60      0.67        15\n",
      "          37       0.28      0.26      0.27        19\n",
      "          38       0.75      0.50      0.60         6\n",
      "          39       0.50      0.36      0.42        11\n",
      "          40       0.35      0.28      0.31        25\n",
      "          41       0.00      0.00      0.00        12\n",
      "          42       0.57      0.36      0.44        11\n",
      "\n",
      "   micro avg       0.39      0.32      0.36       798\n",
      "   macro avg       0.41      0.32      0.35       798\n",
      "weighted avg       0.40      0.32      0.35       798\n",
      " samples avg       0.31      0.34      0.32       798\n",
      "\n",
      "\n",
      "\n",
      "======= SVC2 =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 24 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.39      0.47        31\n",
      "           1       0.36      0.31      0.33        13\n",
      "           2       0.57      0.57      0.57        14\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.30      0.25      0.27        36\n",
      "           5       0.75      0.33      0.46         9\n",
      "           6       0.37      0.39      0.38        28\n",
      "           7       0.13      0.12      0.13        16\n",
      "           8       0.54      0.50      0.52        14\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       1.00      1.00      1.00         6\n",
      "          11       0.30      0.16      0.21        19\n",
      "          12       0.43      0.45      0.44        40\n",
      "          13       0.57      0.42      0.48        19\n",
      "          14       0.33      0.53      0.41        17\n",
      "          15       0.25      0.25      0.25         8\n",
      "          16       0.50      0.17      0.25         6\n",
      "          17       0.48      0.36      0.41        33\n",
      "          18       0.40      0.43      0.41        28\n",
      "          19       0.27      0.30      0.29        47\n",
      "          20       0.46      0.38      0.42        50\n",
      "          21       0.33      0.31      0.32        16\n",
      "          22       0.50      0.40      0.44         5\n",
      "          23       0.58      0.64      0.61        11\n",
      "          24       0.00      0.00      0.00         2\n",
      "          25       0.50      0.39      0.44        28\n",
      "          26       0.10      0.12      0.11         8\n",
      "          27       0.40      0.26      0.31        31\n",
      "          28       0.24      0.19      0.21        27\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       0.60      0.50      0.55        12\n",
      "          31       0.40      0.40      0.40         5\n",
      "          32       0.57      0.44      0.50         9\n",
      "          33       0.41      0.45      0.43        33\n",
      "          34       0.16      0.21      0.18        33\n",
      "          35       0.38      0.25      0.30        36\n",
      "          36       0.82      0.60      0.69        15\n",
      "          37       0.30      0.32      0.31        19\n",
      "          38       0.75      0.50      0.60         6\n",
      "          39       0.44      0.36      0.40        11\n",
      "          40       0.25      0.28      0.26        25\n",
      "          41       0.00      0.00      0.00        12\n",
      "          42       0.57      0.36      0.44        11\n",
      "\n",
      "   micro avg       0.39      0.34      0.36       798\n",
      "   macro avg       0.39      0.33      0.35       798\n",
      "weighted avg       0.40      0.34      0.36       798\n",
      " samples avg       0.32      0.36      0.32       798\n",
      "\n",
      "\n",
      "\n",
      "======= SVC_poly =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 24 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.16      0.26        31\n",
      "           1       0.00      0.00      0.00        13\n",
      "           2       0.67      0.29      0.40        14\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.00      0.00      0.00        36\n",
      "           5       0.00      0.00      0.00         9\n",
      "           6       0.21      0.11      0.14        28\n",
      "           7       0.00      0.00      0.00        16\n",
      "           8       0.56      0.36      0.43        14\n",
      "           9       0.02      0.40      0.03         5\n",
      "          10       1.00      0.67      0.80         6\n",
      "          11       0.17      0.05      0.08        19\n",
      "          12       0.64      0.23      0.33        40\n",
      "          13       0.57      0.21      0.31        19\n",
      "          14       0.24      0.24      0.24        17\n",
      "          15       0.33      0.25      0.29         8\n",
      "          16       0.00      0.00      0.00         6\n",
      "          17       0.27      0.09      0.14        33\n",
      "          18       0.46      0.21      0.29        28\n",
      "          19       0.15      0.04      0.07        47\n",
      "          20       0.43      0.12      0.19        50\n",
      "          21       0.29      0.12      0.17        16\n",
      "          22       1.00      0.20      0.33         5\n",
      "          23       0.70      0.64      0.67        11\n",
      "          24       0.00      0.00      0.00         2\n",
      "          25       0.42      0.18      0.25        28\n",
      "          26       0.00      0.00      0.00         8\n",
      "          27       0.60      0.10      0.17        31\n",
      "          28       0.00      0.00      0.00        27\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.50      0.20      0.29         5\n",
      "          32       0.25      0.11      0.15         9\n",
      "          33       0.50      0.06      0.11        33\n",
      "          34       0.27      0.12      0.17        33\n",
      "          35       0.62      0.14      0.23        36\n",
      "          36       1.00      0.53      0.70        15\n",
      "          37       0.40      0.11      0.17        19\n",
      "          38       0.67      0.33      0.44         6\n",
      "          39       0.67      0.36      0.47        11\n",
      "          40       0.36      0.16      0.22        25\n",
      "          41       0.00      0.00      0.00        12\n",
      "          42       0.40      0.18      0.25        11\n",
      "\n",
      "   micro avg       0.28      0.14      0.19       798\n",
      "   macro avg       0.35      0.16      0.20       798\n",
      "weighted avg       0.37      0.14      0.19       798\n",
      " samples avg       0.12      0.14      0.12       798\n",
      "\n",
      "\n",
      "\n",
      "======= SVC_lin =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 24 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.52      0.52        31\n",
      "           1       0.24      0.31      0.27        13\n",
      "           2       0.57      0.57      0.57        14\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.25      0.39      0.30        36\n",
      "           5       0.38      0.33      0.35         9\n",
      "           6       0.27      0.46      0.34        28\n",
      "           7       0.10      0.19      0.13        16\n",
      "           8       0.56      0.64      0.60        14\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       1.00      1.00      1.00         6\n",
      "          11       0.22      0.21      0.22        19\n",
      "          12       0.35      0.57      0.43        40\n",
      "          13       0.55      0.58      0.56        19\n",
      "          14       0.27      0.47      0.34        17\n",
      "          15       0.27      0.38      0.32         8\n",
      "          16       0.17      0.17      0.17         6\n",
      "          17       0.33      0.45      0.38        33\n",
      "          18       0.42      0.54      0.47        28\n",
      "          19       0.19      0.36      0.25        47\n",
      "          20       0.38      0.46      0.41        50\n",
      "          21       0.31      0.31      0.31        16\n",
      "          22       0.50      0.40      0.44         5\n",
      "          23       0.47      0.64      0.54        11\n",
      "          24       0.00      0.00      0.00         2\n",
      "          25       0.27      0.46      0.34        28\n",
      "          26       0.12      0.12      0.12         8\n",
      "          27       0.28      0.32      0.30        31\n",
      "          28       0.19      0.26      0.22        27\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       0.45      0.42      0.43        12\n",
      "          31       0.38      0.60      0.46         5\n",
      "          32       0.36      0.56      0.43         9\n",
      "          33       0.30      0.48      0.37        33\n",
      "          34       0.12      0.27      0.16        33\n",
      "          35       0.17      0.28      0.21        36\n",
      "          36       0.77      0.67      0.71        15\n",
      "          37       0.35      0.42      0.38        19\n",
      "          38       0.75      0.50      0.60         6\n",
      "          39       0.25      0.36      0.30        11\n",
      "          40       0.12      0.28      0.17        25\n",
      "          41       0.12      0.17      0.14        12\n",
      "          42       0.38      0.27      0.32        11\n",
      "\n",
      "   micro avg       0.29      0.41      0.34       798\n",
      "   macro avg       0.32      0.38      0.34       798\n",
      "weighted avg       0.31      0.41      0.35       798\n",
      " samples avg       0.28      0.42      0.32       798\n",
      "\n",
      "\n",
      "\n",
      "======= SVC_best =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 24 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.58      0.54        31\n",
      "           1       0.19      0.31      0.24        13\n",
      "           2       0.57      0.57      0.57        14\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.25      0.42      0.32        36\n",
      "           5       0.30      0.33      0.32         9\n",
      "           6       0.28      0.46      0.35        28\n",
      "           7       0.10      0.19      0.13        16\n",
      "           8       0.56      0.64      0.60        14\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       1.00      1.00      1.00         6\n",
      "          11       0.26      0.26      0.26        19\n",
      "          12       0.35      0.60      0.44        40\n",
      "          13       0.55      0.58      0.56        19\n",
      "          14       0.26      0.53      0.35        17\n",
      "          15       0.25      0.38      0.30         8\n",
      "          16       0.17      0.17      0.17         6\n",
      "          17       0.33      0.55      0.41        33\n",
      "          18       0.41      0.57      0.48        28\n",
      "          19       0.17      0.38      0.24        47\n",
      "          20       0.35      0.46      0.40        50\n",
      "          21       0.25      0.31      0.28        16\n",
      "          22       0.40      0.40      0.40         5\n",
      "          23       0.47      0.64      0.54        11\n",
      "          24       0.00      0.00      0.00         2\n",
      "          25       0.27      0.50      0.35        28\n",
      "          26       0.11      0.12      0.12         8\n",
      "          27       0.24      0.32      0.28        31\n",
      "          28       0.18      0.26      0.21        27\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       0.45      0.42      0.43        12\n",
      "          31       0.29      0.40      0.33         5\n",
      "          32       0.33      0.56      0.42         9\n",
      "          33       0.29      0.45      0.35        33\n",
      "          34       0.13      0.33      0.18        33\n",
      "          35       0.15      0.31      0.20        36\n",
      "          36       0.77      0.67      0.71        15\n",
      "          37       0.30      0.42      0.35        19\n",
      "          38       0.75      0.50      0.60         6\n",
      "          39       0.24      0.36      0.29        11\n",
      "          40       0.12      0.28      0.17        25\n",
      "          41       0.15      0.25      0.19        12\n",
      "          42       0.31      0.36      0.33        11\n",
      "\n",
      "   micro avg       0.27      0.43      0.33       798\n",
      "   macro avg       0.30      0.39      0.34       798\n",
      "weighted avg       0.30      0.43      0.35       798\n",
      " samples avg       0.27      0.44      0.32       798\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# MULTILABEL with other classifiers with ONEVSREST and weight classes\n",
    "clf_OvsR_cb = {\n",
    "'SVC': OneVsRestClassifier(SVC(C=10, gamma=1, kernel=\"rbf\", decision_function_shape='ovo',class_weight=\"balanced\")),\n",
    "'SVC2': OneVsRestClassifier(SVC(C=5, gamma=1, kernel=\"rbf\", decision_function_shape='ovo',class_weight=\"balanced\")),\n",
    "'SVC_poly': OneVsRestClassifier(SVC(C=10, gamma=1, kernel=\"poly\", degree=5, decision_function_shape='ovo',class_weight=\"balanced\")),\n",
    "'SVC_lin': OneVsRestClassifier(SVC(C=10, gamma=1, kernel=\"linear\", decision_function_shape='ovo',class_weight=\"balanced\")),\n",
    "'SVC_best': OneVsRestClassifier(SVC(C=5, gamma=\"auto\", kernel=\"linear\", decision_function_shape='ovo',class_weight=\"balanced\"))\n",
    "}\n",
    "\n",
    "run_classifiers(clf_OvsR_cb, X, Y_mlb, pipeline2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "### KERAS MLP with class balanced ###\n",
    "\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will set the model for calling it with the Scikit learn API\n",
    "\n",
    "np.random.seed(1)\n",
    "epochs = 200\n",
    "batch_size = 10\n",
    "alpha=1e-5\n",
    "\n",
    "# Create MLP model\n",
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(100, input_dim=input_dim, kernel_regularizer=regularizers.l2(1e-5),\n",
    "                   activity_regularizer=regularizers.l2(1e-5)))\n",
    "    model.add(Dropout(0.15))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # Final layer\n",
    "    model.add(Dense(len(Y_mlb[0])))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weights\n",
    "class_weights = np.apply_along_axis(np.count_nonzero, 0, Y_mlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = create_model(input_dim=len(X_proc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data\n",
    "X_proc = pipeline2.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split between training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_proc, Y_mlb, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 524 samples, validate on 132 samples\n",
      "Epoch 1/200\n",
      "524/524 [==============================] - 10s 19ms/step - loss: 0.6816 - categorical_accuracy: 0.0286 - val_loss: 0.6625 - val_categorical_accuracy: 0.0379\n",
      "Epoch 2/200\n",
      "524/524 [==============================] - 0s 327us/step - loss: 0.6415 - categorical_accuracy: 0.0324 - val_loss: 0.6159 - val_categorical_accuracy: 0.0303\n",
      "Epoch 3/200\n",
      "524/524 [==============================] - 0s 302us/step - loss: 0.5826 - categorical_accuracy: 0.0401 - val_loss: 0.5446 - val_categorical_accuracy: 0.0379\n",
      "Epoch 4/200\n",
      "524/524 [==============================] - 0s 271us/step - loss: 0.4965 - categorical_accuracy: 0.0515 - val_loss: 0.4485 - val_categorical_accuracy: 0.0455\n",
      "Epoch 5/200\n",
      "524/524 [==============================] - 0s 331us/step - loss: 0.3897 - categorical_accuracy: 0.0382 - val_loss: 0.3452 - val_categorical_accuracy: 0.0303\n",
      "Epoch 6/200\n",
      "524/524 [==============================] - 0s 328us/step - loss: 0.2906 - categorical_accuracy: 0.0363 - val_loss: 0.2576 - val_categorical_accuracy: 0.0227\n",
      "Epoch 7/200\n",
      "524/524 [==============================] - 0s 305us/step - loss: 0.2157 - categorical_accuracy: 0.0324 - val_loss: 0.2001 - val_categorical_accuracy: 0.0227\n",
      "Epoch 8/200\n",
      "524/524 [==============================] - 0s 321us/step - loss: 0.1712 - categorical_accuracy: 0.0401 - val_loss: 0.1680 - val_categorical_accuracy: 0.0152\n",
      "Epoch 9/200\n",
      "524/524 [==============================] - 0s 312us/step - loss: 0.1484 - categorical_accuracy: 0.0668 - val_loss: 0.1513 - val_categorical_accuracy: 0.0530\n",
      "Epoch 10/200\n",
      "524/524 [==============================] - 0s 301us/step - loss: 0.1376 - categorical_accuracy: 0.0668 - val_loss: 0.1429 - val_categorical_accuracy: 0.0455\n",
      "Epoch 11/200\n",
      "524/524 [==============================] - 0s 324us/step - loss: 0.1315 - categorical_accuracy: 0.0897 - val_loss: 0.1383 - val_categorical_accuracy: 0.0606\n",
      "Epoch 12/200\n",
      "524/524 [==============================] - 0s 316us/step - loss: 0.1283 - categorical_accuracy: 0.0897 - val_loss: 0.1358 - val_categorical_accuracy: 0.0606\n",
      "Epoch 13/200\n",
      "524/524 [==============================] - 0s 322us/step - loss: 0.1262 - categorical_accuracy: 0.1050 - val_loss: 0.1341 - val_categorical_accuracy: 0.0606\n",
      "Epoch 14/200\n",
      "524/524 [==============================] - 0s 336us/step - loss: 0.1248 - categorical_accuracy: 0.0897 - val_loss: 0.1331 - val_categorical_accuracy: 0.0682\n",
      "Epoch 15/200\n",
      "524/524 [==============================] - 0s 317us/step - loss: 0.1241 - categorical_accuracy: 0.1031 - val_loss: 0.1323 - val_categorical_accuracy: 0.0682\n",
      "Epoch 16/200\n",
      "524/524 [==============================] - 0s 332us/step - loss: 0.1232 - categorical_accuracy: 0.0935 - val_loss: 0.1317 - val_categorical_accuracy: 0.0758\n",
      "Epoch 17/200\n",
      "524/524 [==============================] - 0s 327us/step - loss: 0.1222 - categorical_accuracy: 0.1126 - val_loss: 0.1312 - val_categorical_accuracy: 0.0758\n",
      "Epoch 18/200\n",
      "524/524 [==============================] - 0s 259us/step - loss: 0.1215 - categorical_accuracy: 0.1279 - val_loss: 0.1308 - val_categorical_accuracy: 0.0758\n",
      "Epoch 19/200\n",
      "524/524 [==============================] - 0s 306us/step - loss: 0.1212 - categorical_accuracy: 0.1145 - val_loss: 0.1305 - val_categorical_accuracy: 0.0758\n",
      "Epoch 20/200\n",
      "524/524 [==============================] - 0s 348us/step - loss: 0.1207 - categorical_accuracy: 0.1317 - val_loss: 0.1301 - val_categorical_accuracy: 0.0833\n",
      "Epoch 21/200\n",
      "524/524 [==============================] - 0s 302us/step - loss: 0.1202 - categorical_accuracy: 0.1336 - val_loss: 0.1299 - val_categorical_accuracy: 0.0833\n",
      "Epoch 22/200\n",
      "524/524 [==============================] - 0s 296us/step - loss: 0.1200 - categorical_accuracy: 0.1279 - val_loss: 0.1296 - val_categorical_accuracy: 0.0833\n",
      "Epoch 23/200\n",
      "524/524 [==============================] - 0s 308us/step - loss: 0.1196 - categorical_accuracy: 0.1374 - val_loss: 0.1294 - val_categorical_accuracy: 0.0833\n",
      "Epoch 24/200\n",
      "524/524 [==============================] - 0s 285us/step - loss: 0.1185 - categorical_accuracy: 0.1393 - val_loss: 0.1291 - val_categorical_accuracy: 0.0833\n",
      "Epoch 25/200\n",
      "524/524 [==============================] - 0s 329us/step - loss: 0.1184 - categorical_accuracy: 0.1469 - val_loss: 0.1289 - val_categorical_accuracy: 0.0758\n",
      "Epoch 26/200\n",
      "524/524 [==============================] - 0s 315us/step - loss: 0.1180 - categorical_accuracy: 0.1450 - val_loss: 0.1286 - val_categorical_accuracy: 0.0833\n",
      "Epoch 27/200\n",
      "524/524 [==============================] - 0s 331us/step - loss: 0.1173 - categorical_accuracy: 0.1546 - val_loss: 0.1284 - val_categorical_accuracy: 0.0833\n",
      "Epoch 28/200\n",
      "524/524 [==============================] - 0s 300us/step - loss: 0.1170 - categorical_accuracy: 0.1660 - val_loss: 0.1281 - val_categorical_accuracy: 0.0833\n",
      "Epoch 29/200\n",
      "524/524 [==============================] - 0s 266us/step - loss: 0.1166 - categorical_accuracy: 0.1870 - val_loss: 0.1277 - val_categorical_accuracy: 0.0909\n",
      "Epoch 30/200\n",
      "524/524 [==============================] - 0s 303us/step - loss: 0.1159 - categorical_accuracy: 0.1966 - val_loss: 0.1275 - val_categorical_accuracy: 0.1061\n",
      "Epoch 31/200\n",
      "524/524 [==============================] - 0s 296us/step - loss: 0.1153 - categorical_accuracy: 0.2042 - val_loss: 0.1273 - val_categorical_accuracy: 0.1061\n",
      "Epoch 32/200\n",
      "524/524 [==============================] - 0s 306us/step - loss: 0.1147 - categorical_accuracy: 0.2042 - val_loss: 0.1270 - val_categorical_accuracy: 0.1061\n",
      "Epoch 33/200\n",
      "524/524 [==============================] - 0s 286us/step - loss: 0.1142 - categorical_accuracy: 0.2023 - val_loss: 0.1267 - val_categorical_accuracy: 0.0985\n",
      "Epoch 34/200\n",
      "524/524 [==============================] - 0s 309us/step - loss: 0.1137 - categorical_accuracy: 0.2118 - val_loss: 0.1265 - val_categorical_accuracy: 0.1061\n",
      "Epoch 35/200\n",
      "524/524 [==============================] - 0s 356us/step - loss: 0.1131 - categorical_accuracy: 0.2309 - val_loss: 0.1261 - val_categorical_accuracy: 0.1136\n",
      "Epoch 36/200\n",
      "524/524 [==============================] - 0s 305us/step - loss: 0.1124 - categorical_accuracy: 0.2252 - val_loss: 0.1258 - val_categorical_accuracy: 0.1136\n",
      "Epoch 37/200\n",
      "524/524 [==============================] - 0s 299us/step - loss: 0.1117 - categorical_accuracy: 0.2538 - val_loss: 0.1255 - val_categorical_accuracy: 0.1212\n",
      "Epoch 38/200\n",
      "524/524 [==============================] - 0s 317us/step - loss: 0.1111 - categorical_accuracy: 0.2576 - val_loss: 0.1253 - val_categorical_accuracy: 0.1364\n",
      "Epoch 39/200\n",
      "524/524 [==============================] - 0s 288us/step - loss: 0.1104 - categorical_accuracy: 0.2767 - val_loss: 0.1250 - val_categorical_accuracy: 0.1364\n",
      "Epoch 40/200\n",
      "524/524 [==============================] - 0s 303us/step - loss: 0.1097 - categorical_accuracy: 0.2576 - val_loss: 0.1246 - val_categorical_accuracy: 0.1288\n",
      "Epoch 41/200\n",
      "524/524 [==============================] - 0s 291us/step - loss: 0.1087 - categorical_accuracy: 0.2996 - val_loss: 0.1242 - val_categorical_accuracy: 0.1364\n",
      "Epoch 42/200\n",
      "524/524 [==============================] - 0s 314us/step - loss: 0.1083 - categorical_accuracy: 0.2977 - val_loss: 0.1239 - val_categorical_accuracy: 0.1364\n",
      "Epoch 43/200\n",
      "524/524 [==============================] - 0s 312us/step - loss: 0.1074 - categorical_accuracy: 0.3015 - val_loss: 0.1234 - val_categorical_accuracy: 0.1439\n",
      "Epoch 44/200\n",
      "524/524 [==============================] - 0s 295us/step - loss: 0.1065 - categorical_accuracy: 0.3282 - val_loss: 0.1230 - val_categorical_accuracy: 0.1742\n",
      "Epoch 45/200\n",
      "524/524 [==============================] - 0s 310us/step - loss: 0.1061 - categorical_accuracy: 0.3473 - val_loss: 0.1226 - val_categorical_accuracy: 0.1742\n",
      "Epoch 46/200\n",
      "524/524 [==============================] - 0s 291us/step - loss: 0.1050 - categorical_accuracy: 0.3588 - val_loss: 0.1223 - val_categorical_accuracy: 0.1667\n",
      "Epoch 47/200\n",
      "524/524 [==============================] - 0s 296us/step - loss: 0.1045 - categorical_accuracy: 0.3588 - val_loss: 0.1219 - val_categorical_accuracy: 0.1742\n",
      "Epoch 48/200\n",
      "524/524 [==============================] - 0s 319us/step - loss: 0.1041 - categorical_accuracy: 0.3760 - val_loss: 0.1216 - val_categorical_accuracy: 0.1894\n",
      "Epoch 49/200\n",
      "524/524 [==============================] - 0s 314us/step - loss: 0.1030 - categorical_accuracy: 0.3989 - val_loss: 0.1211 - val_categorical_accuracy: 0.1818\n",
      "Epoch 50/200\n",
      "524/524 [==============================] - 0s 281us/step - loss: 0.1023 - categorical_accuracy: 0.4008 - val_loss: 0.1206 - val_categorical_accuracy: 0.1894\n",
      "Epoch 51/200\n",
      "524/524 [==============================] - 0s 329us/step - loss: 0.1009 - categorical_accuracy: 0.4027 - val_loss: 0.1202 - val_categorical_accuracy: 0.1894\n",
      "Epoch 52/200\n",
      "524/524 [==============================] - 0s 301us/step - loss: 0.1003 - categorical_accuracy: 0.4275 - val_loss: 0.1198 - val_categorical_accuracy: 0.2045\n",
      "Epoch 53/200\n",
      "524/524 [==============================] - 0s 324us/step - loss: 0.0992 - categorical_accuracy: 0.4504 - val_loss: 0.1194 - val_categorical_accuracy: 0.2348\n",
      "Epoch 54/200\n",
      "524/524 [==============================] - 0s 369us/step - loss: 0.0990 - categorical_accuracy: 0.4427 - val_loss: 0.1190 - val_categorical_accuracy: 0.2197\n",
      "Epoch 55/200\n",
      "524/524 [==============================] - 0s 326us/step - loss: 0.0978 - categorical_accuracy: 0.4523 - val_loss: 0.1186 - val_categorical_accuracy: 0.2348\n",
      "Epoch 56/200\n",
      "524/524 [==============================] - 0s 277us/step - loss: 0.0972 - categorical_accuracy: 0.4599 - val_loss: 0.1181 - val_categorical_accuracy: 0.2500\n",
      "Epoch 57/200\n",
      "524/524 [==============================] - 0s 320us/step - loss: 0.0958 - categorical_accuracy: 0.4943 - val_loss: 0.1178 - val_categorical_accuracy: 0.2424\n",
      "Epoch 58/200\n",
      "524/524 [==============================] - 0s 288us/step - loss: 0.0952 - categorical_accuracy: 0.4790 - val_loss: 0.1173 - val_categorical_accuracy: 0.2500\n",
      "Epoch 59/200\n",
      "524/524 [==============================] - 0s 290us/step - loss: 0.0945 - categorical_accuracy: 0.4828 - val_loss: 0.1169 - val_categorical_accuracy: 0.2576\n",
      "Epoch 60/200\n",
      "524/524 [==============================] - 0s 296us/step - loss: 0.0940 - categorical_accuracy: 0.4924 - val_loss: 0.1166 - val_categorical_accuracy: 0.2576\n",
      "Epoch 61/200\n",
      "524/524 [==============================] - 0s 338us/step - loss: 0.0929 - categorical_accuracy: 0.5115 - val_loss: 0.1163 - val_categorical_accuracy: 0.2500\n",
      "Epoch 62/200\n",
      "524/524 [==============================] - 0s 320us/step - loss: 0.0918 - categorical_accuracy: 0.5095 - val_loss: 0.1159 - val_categorical_accuracy: 0.2576\n",
      "Epoch 63/200\n",
      "524/524 [==============================] - 0s 288us/step - loss: 0.0911 - categorical_accuracy: 0.5229 - val_loss: 0.1154 - val_categorical_accuracy: 0.2727\n",
      "Epoch 64/200\n",
      "524/524 [==============================] - 0s 338us/step - loss: 0.0906 - categorical_accuracy: 0.5267 - val_loss: 0.1150 - val_categorical_accuracy: 0.2500\n",
      "Epoch 65/200\n",
      "524/524 [==============================] - 0s 304us/step - loss: 0.0894 - categorical_accuracy: 0.5210 - val_loss: 0.1148 - val_categorical_accuracy: 0.2500\n",
      "Epoch 66/200\n",
      "524/524 [==============================] - 0s 304us/step - loss: 0.0885 - categorical_accuracy: 0.5477 - val_loss: 0.1143 - val_categorical_accuracy: 0.2727\n",
      "Epoch 67/200\n",
      "524/524 [==============================] - 0s 303us/step - loss: 0.0878 - categorical_accuracy: 0.5382 - val_loss: 0.1139 - val_categorical_accuracy: 0.2500\n",
      "Epoch 68/200\n",
      "524/524 [==============================] - 0s 290us/step - loss: 0.0870 - categorical_accuracy: 0.5553 - val_loss: 0.1137 - val_categorical_accuracy: 0.2652\n",
      "Epoch 69/200\n",
      "524/524 [==============================] - 0s 287us/step - loss: 0.0861 - categorical_accuracy: 0.5439 - val_loss: 0.1133 - val_categorical_accuracy: 0.2576\n",
      "Epoch 70/200\n",
      "524/524 [==============================] - 0s 294us/step - loss: 0.0853 - categorical_accuracy: 0.5649 - val_loss: 0.1130 - val_categorical_accuracy: 0.2652\n",
      "Epoch 71/200\n",
      "524/524 [==============================] - 0s 321us/step - loss: 0.0847 - categorical_accuracy: 0.5573 - val_loss: 0.1128 - val_categorical_accuracy: 0.2727\n",
      "Epoch 72/200\n",
      "524/524 [==============================] - 0s 288us/step - loss: 0.0844 - categorical_accuracy: 0.5763 - val_loss: 0.1125 - val_categorical_accuracy: 0.2803\n",
      "Epoch 73/200\n",
      "524/524 [==============================] - 0s 269us/step - loss: 0.0838 - categorical_accuracy: 0.5630 - val_loss: 0.1121 - val_categorical_accuracy: 0.2803\n",
      "Epoch 74/200\n",
      "524/524 [==============================] - 0s 316us/step - loss: 0.0825 - categorical_accuracy: 0.5649 - val_loss: 0.1118 - val_categorical_accuracy: 0.2727\n",
      "Epoch 75/200\n",
      "524/524 [==============================] - 0s 310us/step - loss: 0.0823 - categorical_accuracy: 0.5573 - val_loss: 0.1117 - val_categorical_accuracy: 0.2803\n",
      "Epoch 76/200\n",
      "524/524 [==============================] - 0s 311us/step - loss: 0.0813 - categorical_accuracy: 0.5687 - val_loss: 0.1114 - val_categorical_accuracy: 0.2803\n",
      "Epoch 77/200\n",
      "524/524 [==============================] - 0s 306us/step - loss: 0.0805 - categorical_accuracy: 0.5687 - val_loss: 0.1113 - val_categorical_accuracy: 0.2803\n",
      "Epoch 78/200\n",
      "524/524 [==============================] - 0s 304us/step - loss: 0.0798 - categorical_accuracy: 0.5821 - val_loss: 0.1110 - val_categorical_accuracy: 0.2879\n",
      "Epoch 79/200\n",
      "524/524 [==============================] - 0s 263us/step - loss: 0.0792 - categorical_accuracy: 0.5954 - val_loss: 0.1107 - val_categorical_accuracy: 0.2879\n",
      "Epoch 80/200\n",
      "524/524 [==============================] - 0s 317us/step - loss: 0.0786 - categorical_accuracy: 0.6011 - val_loss: 0.1105 - val_categorical_accuracy: 0.2879\n",
      "Epoch 81/200\n",
      "524/524 [==============================] - 0s 313us/step - loss: 0.0783 - categorical_accuracy: 0.5802 - val_loss: 0.1101 - val_categorical_accuracy: 0.2803\n",
      "Epoch 82/200\n",
      "524/524 [==============================] - 0s 308us/step - loss: 0.0770 - categorical_accuracy: 0.6011 - val_loss: 0.1099 - val_categorical_accuracy: 0.2803\n",
      "Epoch 83/200\n",
      "524/524 [==============================] - 0s 313us/step - loss: 0.0765 - categorical_accuracy: 0.6221 - val_loss: 0.1098 - val_categorical_accuracy: 0.2803\n",
      "Epoch 84/200\n",
      "524/524 [==============================] - 0s 289us/step - loss: 0.0768 - categorical_accuracy: 0.6050 - val_loss: 0.1096 - val_categorical_accuracy: 0.2803\n",
      "Epoch 85/200\n",
      "524/524 [==============================] - 0s 319us/step - loss: 0.0757 - categorical_accuracy: 0.6088 - val_loss: 0.1095 - val_categorical_accuracy: 0.2955\n",
      "Epoch 86/200\n",
      "524/524 [==============================] - 0s 309us/step - loss: 0.0752 - categorical_accuracy: 0.6107 - val_loss: 0.1093 - val_categorical_accuracy: 0.2879\n",
      "Epoch 87/200\n",
      "524/524 [==============================] - 0s 285us/step - loss: 0.0747 - categorical_accuracy: 0.6126 - val_loss: 0.1092 - val_categorical_accuracy: 0.2803\n",
      "Epoch 88/200\n",
      "524/524 [==============================] - 0s 314us/step - loss: 0.0743 - categorical_accuracy: 0.6164 - val_loss: 0.1090 - val_categorical_accuracy: 0.2879\n",
      "Epoch 89/200\n",
      "524/524 [==============================] - 0s 311us/step - loss: 0.0739 - categorical_accuracy: 0.6202 - val_loss: 0.1089 - val_categorical_accuracy: 0.2879\n",
      "Epoch 90/200\n",
      "524/524 [==============================] - 0s 316us/step - loss: 0.0730 - categorical_accuracy: 0.6260 - val_loss: 0.1088 - val_categorical_accuracy: 0.2879\n",
      "Epoch 91/200\n",
      "524/524 [==============================] - 0s 304us/step - loss: 0.0722 - categorical_accuracy: 0.6374 - val_loss: 0.1086 - val_categorical_accuracy: 0.2879\n",
      "Epoch 92/200\n",
      "524/524 [==============================] - 0s 311us/step - loss: 0.0722 - categorical_accuracy: 0.6298 - val_loss: 0.1084 - val_categorical_accuracy: 0.2803\n",
      "Epoch 93/200\n",
      "524/524 [==============================] - 0s 281us/step - loss: 0.0717 - categorical_accuracy: 0.6374 - val_loss: 0.1083 - val_categorical_accuracy: 0.2879\n",
      "Epoch 94/200\n",
      "524/524 [==============================] - 0s 258us/step - loss: 0.0711 - categorical_accuracy: 0.6183 - val_loss: 0.1083 - val_categorical_accuracy: 0.2803\n",
      "Epoch 95/200\n",
      "524/524 [==============================] - 0s 291us/step - loss: 0.0704 - categorical_accuracy: 0.6412 - val_loss: 0.1081 - val_categorical_accuracy: 0.2879\n",
      "Epoch 96/200\n",
      "524/524 [==============================] - 0s 295us/step - loss: 0.0702 - categorical_accuracy: 0.6260 - val_loss: 0.1081 - val_categorical_accuracy: 0.2879\n",
      "Epoch 97/200\n",
      "524/524 [==============================] - 0s 316us/step - loss: 0.0699 - categorical_accuracy: 0.6374 - val_loss: 0.1080 - val_categorical_accuracy: 0.2879\n",
      "Epoch 98/200\n",
      "524/524 [==============================] - 0s 287us/step - loss: 0.0691 - categorical_accuracy: 0.6527 - val_loss: 0.1079 - val_categorical_accuracy: 0.2879\n",
      "Epoch 99/200\n",
      "524/524 [==============================] - 0s 297us/step - loss: 0.0689 - categorical_accuracy: 0.6546 - val_loss: 0.1078 - val_categorical_accuracy: 0.2879\n",
      "Epoch 100/200\n",
      "524/524 [==============================] - 0s 310us/step - loss: 0.0681 - categorical_accuracy: 0.6260 - val_loss: 0.1078 - val_categorical_accuracy: 0.2879\n",
      "Epoch 101/200\n",
      "524/524 [==============================] - 0s 313us/step - loss: 0.0686 - categorical_accuracy: 0.6412 - val_loss: 0.1078 - val_categorical_accuracy: 0.2879\n",
      "Epoch 102/200\n",
      "524/524 [==============================] - 0s 286us/step - loss: 0.0679 - categorical_accuracy: 0.6584 - val_loss: 0.1078 - val_categorical_accuracy: 0.3030\n",
      "Epoch 103/200\n",
      "524/524 [==============================] - 0s 289us/step - loss: 0.0671 - categorical_accuracy: 0.6718 - val_loss: 0.1076 - val_categorical_accuracy: 0.3030\n",
      "Epoch 104/200\n",
      "524/524 [==============================] - 0s 290us/step - loss: 0.0669 - categorical_accuracy: 0.6698 - val_loss: 0.1077 - val_categorical_accuracy: 0.2879\n",
      "Epoch 105/200\n",
      "524/524 [==============================] - 0s 281us/step - loss: 0.0671 - categorical_accuracy: 0.6489 - val_loss: 0.1076 - val_categorical_accuracy: 0.2955\n",
      "Epoch 106/200\n",
      "524/524 [==============================] - 0s 271us/step - loss: 0.0657 - categorical_accuracy: 0.6756 - val_loss: 0.1076 - val_categorical_accuracy: 0.3030\n",
      "Epoch 107/200\n",
      "524/524 [==============================] - 0s 313us/step - loss: 0.0662 - categorical_accuracy: 0.6584 - val_loss: 0.1074 - val_categorical_accuracy: 0.3106\n",
      "Epoch 108/200\n",
      "524/524 [==============================] - 0s 299us/step - loss: 0.0657 - categorical_accuracy: 0.6489 - val_loss: 0.1075 - val_categorical_accuracy: 0.3182\n",
      "Epoch 109/200\n",
      "524/524 [==============================] - 0s 302us/step - loss: 0.0649 - categorical_accuracy: 0.6737 - val_loss: 0.1076 - val_categorical_accuracy: 0.3333\n",
      "Epoch 110/200\n",
      "524/524 [==============================] - 0s 306us/step - loss: 0.0646 - categorical_accuracy: 0.6698 - val_loss: 0.1074 - val_categorical_accuracy: 0.3409\n",
      "Epoch 111/200\n",
      "524/524 [==============================] - 0s 299us/step - loss: 0.0643 - categorical_accuracy: 0.6870 - val_loss: 0.1075 - val_categorical_accuracy: 0.3409\n",
      "Epoch 112/200\n",
      "524/524 [==============================] - 0s 294us/step - loss: 0.0642 - categorical_accuracy: 0.6660 - val_loss: 0.1076 - val_categorical_accuracy: 0.3258\n",
      "Epoch 113/200\n",
      "524/524 [==============================] - 0s 288us/step - loss: 0.0635 - categorical_accuracy: 0.6794 - val_loss: 0.1076 - val_categorical_accuracy: 0.3333\n",
      "Epoch 114/200\n",
      "524/524 [==============================] - 0s 270us/step - loss: 0.0630 - categorical_accuracy: 0.6908 - val_loss: 0.1075 - val_categorical_accuracy: 0.3409\n",
      "Epoch 115/200\n",
      "524/524 [==============================] - 0s 311us/step - loss: 0.0628 - categorical_accuracy: 0.6794 - val_loss: 0.1074 - val_categorical_accuracy: 0.3409\n",
      "Epoch 116/200\n",
      "524/524 [==============================] - 0s 288us/step - loss: 0.0627 - categorical_accuracy: 0.6718 - val_loss: 0.1075 - val_categorical_accuracy: 0.3333\n",
      "Epoch 117/200\n",
      "524/524 [==============================] - 0s 284us/step - loss: 0.0626 - categorical_accuracy: 0.6813 - val_loss: 0.1075 - val_categorical_accuracy: 0.3333\n",
      "Epoch 118/200\n",
      "524/524 [==============================] - 0s 338us/step - loss: 0.0621 - categorical_accuracy: 0.6813 - val_loss: 0.1075 - val_categorical_accuracy: 0.3258\n",
      "Epoch 119/200\n",
      "524/524 [==============================] - 0s 319us/step - loss: 0.0619 - categorical_accuracy: 0.7118 - val_loss: 0.1074 - val_categorical_accuracy: 0.3258\n",
      "Epoch 120/200\n",
      "524/524 [==============================] - 0s 327us/step - loss: 0.0609 - categorical_accuracy: 0.6908 - val_loss: 0.1076 - val_categorical_accuracy: 0.3333\n",
      "Epoch 121/200\n",
      "524/524 [==============================] - 0s 272us/step - loss: 0.0607 - categorical_accuracy: 0.7099 - val_loss: 0.1077 - val_categorical_accuracy: 0.3182\n",
      "Epoch 122/200\n",
      "524/524 [==============================] - 0s 312us/step - loss: 0.0614 - categorical_accuracy: 0.6889 - val_loss: 0.1077 - val_categorical_accuracy: 0.3333\n",
      "Epoch 123/200\n",
      "524/524 [==============================] - 0s 306us/step - loss: 0.0604 - categorical_accuracy: 0.7080 - val_loss: 0.1076 - val_categorical_accuracy: 0.3333\n",
      "Epoch 124/200\n",
      "524/524 [==============================] - 0s 310us/step - loss: 0.0598 - categorical_accuracy: 0.7156 - val_loss: 0.1076 - val_categorical_accuracy: 0.3409\n",
      "Epoch 125/200\n",
      "524/524 [==============================] - 0s 290us/step - loss: 0.0598 - categorical_accuracy: 0.7176 - val_loss: 0.1077 - val_categorical_accuracy: 0.3409\n",
      "Epoch 126/200\n",
      "524/524 [==============================] - 0s 278us/step - loss: 0.0590 - categorical_accuracy: 0.7099 - val_loss: 0.1076 - val_categorical_accuracy: 0.3409\n",
      "Epoch 127/200\n",
      "524/524 [==============================] - 0s 341us/step - loss: 0.0591 - categorical_accuracy: 0.7176 - val_loss: 0.1076 - val_categorical_accuracy: 0.3409\n",
      "Epoch 128/200\n",
      "524/524 [==============================] - 0s 349us/step - loss: 0.0589 - categorical_accuracy: 0.7080 - val_loss: 0.1077 - val_categorical_accuracy: 0.3409\n",
      "Epoch 129/200\n",
      "524/524 [==============================] - 0s 325us/step - loss: 0.0589 - categorical_accuracy: 0.7004 - val_loss: 0.1078 - val_categorical_accuracy: 0.3333\n",
      "Epoch 130/200\n",
      "524/524 [==============================] - 0s 310us/step - loss: 0.0584 - categorical_accuracy: 0.7061 - val_loss: 0.1077 - val_categorical_accuracy: 0.3636\n",
      "Epoch 131/200\n",
      "524/524 [==============================] - 0s 284us/step - loss: 0.0582 - categorical_accuracy: 0.7080 - val_loss: 0.1080 - val_categorical_accuracy: 0.3409\n",
      "Epoch 132/200\n",
      "524/524 [==============================] - 0s 290us/step - loss: 0.0580 - categorical_accuracy: 0.6927 - val_loss: 0.1078 - val_categorical_accuracy: 0.3561\n",
      "Epoch 133/200\n",
      "524/524 [==============================] - 0s 301us/step - loss: 0.0577 - categorical_accuracy: 0.7137 - val_loss: 0.1079 - val_categorical_accuracy: 0.3636\n",
      "Epoch 134/200\n",
      "524/524 [==============================] - 0s 281us/step - loss: 0.0568 - categorical_accuracy: 0.7252 - val_loss: 0.1081 - val_categorical_accuracy: 0.3485\n",
      "Epoch 135/200\n",
      "524/524 [==============================] - 0s 369us/step - loss: 0.0575 - categorical_accuracy: 0.7271 - val_loss: 0.1080 - val_categorical_accuracy: 0.3485\n",
      "Epoch 136/200\n",
      "524/524 [==============================] - 0s 294us/step - loss: 0.0566 - categorical_accuracy: 0.7195 - val_loss: 0.1082 - val_categorical_accuracy: 0.3409\n",
      "Epoch 137/200\n",
      "524/524 [==============================] - 0s 328us/step - loss: 0.0569 - categorical_accuracy: 0.6947 - val_loss: 0.1081 - val_categorical_accuracy: 0.3485\n",
      "Epoch 138/200\n",
      "524/524 [==============================] - 0s 313us/step - loss: 0.0563 - categorical_accuracy: 0.7118 - val_loss: 0.1081 - val_categorical_accuracy: 0.3561\n",
      "Epoch 139/200\n",
      "524/524 [==============================] - 0s 331us/step - loss: 0.0559 - categorical_accuracy: 0.7309 - val_loss: 0.1083 - val_categorical_accuracy: 0.3561\n",
      "Epoch 140/200\n",
      "524/524 [==============================] - 0s 301us/step - loss: 0.0557 - categorical_accuracy: 0.7443 - val_loss: 0.1082 - val_categorical_accuracy: 0.3409\n",
      "Epoch 141/200\n",
      "524/524 [==============================] - 0s 304us/step - loss: 0.0561 - categorical_accuracy: 0.7099 - val_loss: 0.1083 - val_categorical_accuracy: 0.3409\n",
      "Epoch 142/200\n",
      "524/524 [==============================] - 0s 301us/step - loss: 0.0554 - categorical_accuracy: 0.7481 - val_loss: 0.1084 - val_categorical_accuracy: 0.3485\n",
      "Epoch 143/200\n",
      "524/524 [==============================] - 0s 338us/step - loss: 0.0551 - categorical_accuracy: 0.7328 - val_loss: 0.1084 - val_categorical_accuracy: 0.3485\n",
      "Epoch 144/200\n",
      "524/524 [==============================] - 0s 328us/step - loss: 0.0550 - categorical_accuracy: 0.7290 - val_loss: 0.1086 - val_categorical_accuracy: 0.3409\n",
      "Epoch 145/200\n",
      "524/524 [==============================] - 0s 313us/step - loss: 0.0546 - categorical_accuracy: 0.7405 - val_loss: 0.1085 - val_categorical_accuracy: 0.3409\n",
      "Epoch 146/200\n",
      "524/524 [==============================] - 0s 295us/step - loss: 0.0548 - categorical_accuracy: 0.7290 - val_loss: 0.1085 - val_categorical_accuracy: 0.3409\n",
      "Epoch 147/200\n",
      "524/524 [==============================] - 0s 319us/step - loss: 0.0545 - categorical_accuracy: 0.7500 - val_loss: 0.1087 - val_categorical_accuracy: 0.3409\n",
      "Epoch 148/200\n",
      "524/524 [==============================] - 0s 323us/step - loss: 0.0541 - categorical_accuracy: 0.7576 - val_loss: 0.1088 - val_categorical_accuracy: 0.3409\n",
      "Epoch 149/200\n",
      "524/524 [==============================] - 0s 286us/step - loss: 0.0535 - categorical_accuracy: 0.7366 - val_loss: 0.1090 - val_categorical_accuracy: 0.3409\n",
      "Epoch 150/200\n",
      "524/524 [==============================] - 0s 312us/step - loss: 0.0544 - categorical_accuracy: 0.7462 - val_loss: 0.1091 - val_categorical_accuracy: 0.3485\n",
      "Epoch 151/200\n",
      "524/524 [==============================] - 0s 317us/step - loss: 0.0536 - categorical_accuracy: 0.7729 - val_loss: 0.1091 - val_categorical_accuracy: 0.3409\n",
      "Epoch 152/200\n",
      "524/524 [==============================] - 0s 333us/step - loss: 0.0529 - categorical_accuracy: 0.7576 - val_loss: 0.1092 - val_categorical_accuracy: 0.3485\n",
      "Epoch 153/200\n",
      "524/524 [==============================] - 0s 325us/step - loss: 0.0522 - categorical_accuracy: 0.7557 - val_loss: 0.1094 - val_categorical_accuracy: 0.3485\n",
      "Epoch 154/200\n",
      "524/524 [==============================] - 0s 279us/step - loss: 0.0533 - categorical_accuracy: 0.7615 - val_loss: 0.1094 - val_categorical_accuracy: 0.3409\n",
      "Epoch 155/200\n",
      "524/524 [==============================] - 0s 325us/step - loss: 0.0526 - categorical_accuracy: 0.7615 - val_loss: 0.1094 - val_categorical_accuracy: 0.3485\n",
      "Epoch 156/200\n",
      "524/524 [==============================] - 0s 307us/step - loss: 0.0524 - categorical_accuracy: 0.7538 - val_loss: 0.1095 - val_categorical_accuracy: 0.3485\n",
      "Epoch 157/200\n",
      "524/524 [==============================] - 0s 314us/step - loss: 0.0526 - categorical_accuracy: 0.7519 - val_loss: 0.1096 - val_categorical_accuracy: 0.3485\n",
      "Epoch 158/200\n",
      "524/524 [==============================] - 0s 295us/step - loss: 0.0525 - categorical_accuracy: 0.7634 - val_loss: 0.1097 - val_categorical_accuracy: 0.3485\n",
      "Epoch 159/200\n",
      "524/524 [==============================] - 0s 299us/step - loss: 0.0516 - categorical_accuracy: 0.7691 - val_loss: 0.1098 - val_categorical_accuracy: 0.3485\n",
      "Epoch 160/200\n",
      "524/524 [==============================] - 0s 309us/step - loss: 0.0519 - categorical_accuracy: 0.7710 - val_loss: 0.1100 - val_categorical_accuracy: 0.3485\n",
      "Epoch 161/200\n",
      "524/524 [==============================] - 0s 291us/step - loss: 0.0509 - categorical_accuracy: 0.7805 - val_loss: 0.1099 - val_categorical_accuracy: 0.3561\n",
      "Epoch 162/200\n",
      "524/524 [==============================] - 0s 315us/step - loss: 0.0512 - categorical_accuracy: 0.7767 - val_loss: 0.1100 - val_categorical_accuracy: 0.3561\n",
      "Epoch 163/200\n",
      "524/524 [==============================] - 0s 338us/step - loss: 0.0509 - categorical_accuracy: 0.7634 - val_loss: 0.1103 - val_categorical_accuracy: 0.3561\n",
      "Epoch 164/200\n",
      "524/524 [==============================] - 0s 317us/step - loss: 0.0506 - categorical_accuracy: 0.7767 - val_loss: 0.1102 - val_categorical_accuracy: 0.3561\n",
      "Epoch 165/200\n",
      "524/524 [==============================] - 0s 319us/step - loss: 0.0504 - categorical_accuracy: 0.7729 - val_loss: 0.1104 - val_categorical_accuracy: 0.3561\n",
      "Epoch 166/200\n",
      "524/524 [==============================] - 0s 299us/step - loss: 0.0507 - categorical_accuracy: 0.7672 - val_loss: 0.1104 - val_categorical_accuracy: 0.3636\n",
      "Epoch 167/200\n",
      "524/524 [==============================] - 0s 287us/step - loss: 0.0507 - categorical_accuracy: 0.7786 - val_loss: 0.1106 - val_categorical_accuracy: 0.3636\n",
      "Epoch 168/200\n",
      "524/524 [==============================] - 0s 333us/step - loss: 0.0501 - categorical_accuracy: 0.7939 - val_loss: 0.1105 - val_categorical_accuracy: 0.3636\n",
      "Epoch 169/200\n",
      "524/524 [==============================] - 0s 295us/step - loss: 0.0502 - categorical_accuracy: 0.7786 - val_loss: 0.1105 - val_categorical_accuracy: 0.3636\n",
      "Epoch 170/200\n",
      "524/524 [==============================] - 0s 298us/step - loss: 0.0493 - categorical_accuracy: 0.7977 - val_loss: 0.1106 - val_categorical_accuracy: 0.3636\n",
      "Epoch 171/200\n",
      "524/524 [==============================] - 0s 314us/step - loss: 0.0497 - categorical_accuracy: 0.7844 - val_loss: 0.1109 - val_categorical_accuracy: 0.3636\n",
      "Epoch 172/200\n",
      "524/524 [==============================] - 0s 345us/step - loss: 0.0499 - categorical_accuracy: 0.7958 - val_loss: 0.1110 - val_categorical_accuracy: 0.3561\n",
      "Epoch 173/200\n",
      "524/524 [==============================] - 0s 305us/step - loss: 0.0497 - categorical_accuracy: 0.7805 - val_loss: 0.1110 - val_categorical_accuracy: 0.3561\n",
      "Epoch 174/200\n",
      "524/524 [==============================] - 0s 300us/step - loss: 0.0495 - categorical_accuracy: 0.7805 - val_loss: 0.1111 - val_categorical_accuracy: 0.3636\n",
      "Epoch 175/200\n",
      "524/524 [==============================] - 0s 305us/step - loss: 0.0489 - categorical_accuracy: 0.7844 - val_loss: 0.1113 - val_categorical_accuracy: 0.3636\n",
      "Epoch 176/200\n",
      "524/524 [==============================] - 0s 306us/step - loss: 0.0492 - categorical_accuracy: 0.7920 - val_loss: 0.1114 - val_categorical_accuracy: 0.3636\n",
      "Epoch 177/200\n",
      "524/524 [==============================] - 0s 304us/step - loss: 0.0493 - categorical_accuracy: 0.7748 - val_loss: 0.1115 - val_categorical_accuracy: 0.3561\n",
      "Epoch 178/200\n",
      "524/524 [==============================] - 0s 289us/step - loss: 0.0480 - categorical_accuracy: 0.7958 - val_loss: 0.1117 - val_categorical_accuracy: 0.3561\n",
      "Epoch 179/200\n",
      "524/524 [==============================] - 0s 311us/step - loss: 0.0480 - categorical_accuracy: 0.8130 - val_loss: 0.1117 - val_categorical_accuracy: 0.3561\n",
      "Epoch 180/200\n",
      "524/524 [==============================] - 0s 327us/step - loss: 0.0478 - categorical_accuracy: 0.8092 - val_loss: 0.1117 - val_categorical_accuracy: 0.3561\n",
      "Epoch 181/200\n",
      "524/524 [==============================] - 0s 305us/step - loss: 0.0476 - categorical_accuracy: 0.8130 - val_loss: 0.1118 - val_categorical_accuracy: 0.3636\n",
      "Epoch 182/200\n",
      "524/524 [==============================] - 0s 349us/step - loss: 0.0473 - categorical_accuracy: 0.8168 - val_loss: 0.1119 - val_categorical_accuracy: 0.3561\n",
      "Epoch 183/200\n",
      "524/524 [==============================] - 0s 296us/step - loss: 0.0472 - categorical_accuracy: 0.8149 - val_loss: 0.1120 - val_categorical_accuracy: 0.3636\n",
      "Epoch 184/200\n",
      "524/524 [==============================] - 0s 314us/step - loss: 0.0472 - categorical_accuracy: 0.8168 - val_loss: 0.1122 - val_categorical_accuracy: 0.3636\n",
      "Epoch 185/200\n",
      "524/524 [==============================] - 0s 333us/step - loss: 0.0463 - categorical_accuracy: 0.8168 - val_loss: 0.1123 - val_categorical_accuracy: 0.3636\n",
      "Epoch 186/200\n",
      "524/524 [==============================] - 0s 286us/step - loss: 0.0470 - categorical_accuracy: 0.8053 - val_loss: 0.1125 - val_categorical_accuracy: 0.3636\n",
      "Epoch 187/200\n",
      "524/524 [==============================] - 0s 327us/step - loss: 0.0471 - categorical_accuracy: 0.7882 - val_loss: 0.1126 - val_categorical_accuracy: 0.3636\n",
      "Epoch 188/200\n",
      "524/524 [==============================] - 0s 338us/step - loss: 0.0467 - categorical_accuracy: 0.8168 - val_loss: 0.1126 - val_categorical_accuracy: 0.3636\n",
      "Epoch 189/200\n",
      "524/524 [==============================] - 0s 318us/step - loss: 0.0475 - categorical_accuracy: 0.8111 - val_loss: 0.1126 - val_categorical_accuracy: 0.3636\n",
      "Epoch 190/200\n",
      "524/524 [==============================] - 0s 340us/step - loss: 0.0463 - categorical_accuracy: 0.8111 - val_loss: 0.1129 - val_categorical_accuracy: 0.3636\n",
      "Epoch 191/200\n",
      "524/524 [==============================] - 0s 313us/step - loss: 0.0462 - categorical_accuracy: 0.8187 - val_loss: 0.1129 - val_categorical_accuracy: 0.3561\n",
      "Epoch 192/200\n",
      "524/524 [==============================] - 0s 352us/step - loss: 0.0461 - categorical_accuracy: 0.8206 - val_loss: 0.1130 - val_categorical_accuracy: 0.3636\n",
      "Epoch 193/200\n",
      "524/524 [==============================] - 0s 328us/step - loss: 0.0456 - categorical_accuracy: 0.8359 - val_loss: 0.1131 - val_categorical_accuracy: 0.3561\n",
      "Epoch 194/200\n",
      "524/524 [==============================] - 0s 333us/step - loss: 0.0465 - categorical_accuracy: 0.8092 - val_loss: 0.1134 - val_categorical_accuracy: 0.3561\n",
      "Epoch 195/200\n",
      "524/524 [==============================] - 0s 343us/step - loss: 0.0463 - categorical_accuracy: 0.8073 - val_loss: 0.1134 - val_categorical_accuracy: 0.3561\n",
      "Epoch 196/200\n",
      "524/524 [==============================] - 0s 284us/step - loss: 0.0457 - categorical_accuracy: 0.8359 - val_loss: 0.1136 - val_categorical_accuracy: 0.3561\n",
      "Epoch 197/200\n",
      "524/524 [==============================] - 0s 311us/step - loss: 0.0450 - categorical_accuracy: 0.8168 - val_loss: 0.1138 - val_categorical_accuracy: 0.3561\n",
      "Epoch 198/200\n",
      "524/524 [==============================] - 0s 336us/step - loss: 0.0452 - categorical_accuracy: 0.8187 - val_loss: 0.1139 - val_categorical_accuracy: 0.3636\n",
      "Epoch 199/200\n",
      "524/524 [==============================] - 0s 311us/step - loss: 0.0447 - categorical_accuracy: 0.8435 - val_loss: 0.1140 - val_categorical_accuracy: 0.3561\n",
      "Epoch 200/200\n",
      "524/524 [==============================] - 0s 334us/step - loss: 0.0446 - categorical_accuracy: 0.8321 - val_loss: 0.1139 - val_categorical_accuracy: 0.3636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd4d3714cc0>"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "model.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypreds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypreds_int = np.zeros_like(ypreds)\n",
    "ypreds_int[ypreds>=0.5] = 1\n",
    "ypreds_int[ypreds<0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.7931244e-03, 3.9255321e-03, 1.4305204e-02, ..., 1.0032922e-02,\n",
       "        1.1538655e-02, 4.7504604e-03],\n",
       "       [4.3094158e-04, 2.8300285e-04, 8.6039305e-04, ..., 6.5535307e-05,\n",
       "        1.7729402e-04, 3.1057894e-03],\n",
       "       [2.3993820e-02, 2.3756027e-03, 5.2696198e-02, ..., 3.1572282e-03,\n",
       "        4.3440163e-03, 5.6802630e-03],\n",
       "       ...,\n",
       "       [3.0794740e-04, 1.3528466e-03, 2.9908836e-02, ..., 1.3589859e-03,\n",
       "        5.5104494e-04, 8.3342195e-04],\n",
       "       [1.2870729e-03, 4.3887645e-02, 8.8682771e-04, ..., 1.5146434e-02,\n",
       "        4.7052205e-03, 4.9856305e-04],\n",
       "       [8.0657601e-03, 1.7917335e-02, 6.7669153e-04, ..., 1.5256833e-03,\n",
       "        1.1959722e-03, 5.6374277e-04]], dtype=float32)"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3,   3,   7,   9,   9,  12,  13,  16,  18,  18,  21,  23,  24,\n",
       "         24,  24,  25,  31,  34,  39,  40,  41,  42,  42,  43,  45,  46,\n",
       "         50,  50,  51,  53,  54,  56,  57,  60,  61,  64,  65,  66,  67,\n",
       "         72,  74,  75,  77,  80,  92,  92,  96,  97,  99, 100, 106, 110,\n",
       "        114, 120, 122, 123, 124, 129]),\n",
       " array([ 2, 15, 40, 17, 39,  0,  4, 42,  6, 17, 37, 33, 19, 25, 34,  2,  0,\n",
       "        12, 27, 13, 12,  6, 17, 17, 25, 25, 17, 33,  8, 32, 25,  4, 17, 20,\n",
       "        21, 17,  4, 36,  4, 12, 37, 13, 19, 33,  6, 33, 42, 27, 17, 20, 13,\n",
       "        32, 12, 17, 39, 10, 20, 20]))"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(ypreds_int == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2920353982300885"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, ypreds_int, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEST MLP PIPELINE\n",
    "pipeline_mlp = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, min_df=.0025, max_df=0.25, ngram_range=(1,3))),\n",
    "            ('svd', TruncatedSVD(algorithm='randomized', n_components=100, random_state=1)),\n",
    "            ('MLP', MLPClassifier(solver='lbfgs',alpha=1e-5,hidden_layer_sizes=(100),random_state=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, vocabs_train, vocabs_test = train_test_split(X, Y_mlb, vocabs, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.25, max_features=None, min_df=0.0025,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf...True, solver='lbfgs', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False))])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "pipeline_mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline_mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probas = pipeline_mlp.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.61084114e-26, 3.03420474e-28, 1.97892467e-38, ...,\n",
       "        6.45401528e-29, 4.43539277e-38, 1.25558894e-34],\n",
       "       [1.35484764e-13, 1.18487097e-56, 1.98329224e-41, ...,\n",
       "        6.35343769e-11, 4.07530161e-10, 1.88383652e-52],\n",
       "       [1.76924771e-25, 3.42626342e-34, 1.11219954e-49, ...,\n",
       "        2.56725686e-34, 4.22418826e-47, 2.40280405e-37],\n",
       "       ...,\n",
       "       [6.53904933e-52, 8.00590038e-31, 3.38471659e-37, ...,\n",
       "        9.24587112e-28, 2.09201948e-39, 1.20330368e-46],\n",
       "       [3.95278898e-84, 3.23055830e-38, 6.55220352e-72, ...,\n",
       "        1.77668911e-33, 4.14978945e-97, 1.34202954e-86],\n",
       "       [1.16159507e-26, 5.16314801e-25, 2.24881456e-53, ...,\n",
       "        2.80328582e-30, 7.04062310e-55, 2.66813690e-27]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one tag for each prediction by the highest value in the predicted vector\n",
    "row_maxs = y_pred_probas.max(axis=1, keepdims=True)\n",
    "# Indices of maximum value for each row\n",
    "y_pred1 = np.where(y_pred_probas == row_maxs, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Society',), ('RDF',), ('Services',), ('Environment',), ('Services',)]\n",
      "[('API',), ('RDF',), ('Security',), ('Time',), ('Society',)]\n",
      "['wdrs' 'rr' 'algo' 'interval' 'comm']\n"
     ]
    }
   ],
   "source": [
    "# Compare prediction and true label\n",
    "print(mlb.inverse_transform(y_pred1[:5]))\n",
    "print(mlb.inverse_transform(y_test[:5]))\n",
    "print(vocabs_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.54885587e-120 4.02558368e-033 4.97503419e-049 5.88969552e-042\n",
      " 1.53312744e-071 2.28532802e-127 1.70999415e-094 1.06012728e-075\n",
      " 5.04039157e-063 1.63265017e-037 4.36245761e-098 7.02502911e-054\n",
      " 7.30329951e-092 3.08354532e-057 6.37495054e-116 2.31459374e-062\n",
      " 1.27203227e-082 3.03122087e-105 1.59619395e-086 3.35553572e-066\n",
      " 9.99999999e-001 4.90648356e-077 1.08729097e-073 5.49531645e-032\n",
      " 9.07680576e-053 1.21609655e-106 4.90804301e-051 3.36449920e-006\n",
      " 1.10067037e-060 3.58686655e-046 1.89011519e-028 3.55901399e-145\n",
      " 1.06019624e-102 1.47854944e-059 7.19587903e-098 2.99722375e-014\n",
      " 5.51782322e-062 8.25005058e-058 3.03752267e-067 2.98471665e-086\n",
      " 6.40590116e-045 1.28706158e-083 2.42568169e-047]\n",
      "(array([20]),)\n"
     ]
    }
   ],
   "source": [
    "# It seems that labels are predicted if value of neuron > 0.5\n",
    "print(y_pred_probas[4])\n",
    "print(np.where(y_pred[4] == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26459143968871596\n",
      "0.35051546391752575\n",
      "0.25569895221058014\n",
      "0.2125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_test, y_pred, average='micro'))\n",
    "print(precision_score(y_test, y_pred, average=\"micro\"))\n",
    "print(precision_score(y_test, y_pred, average=\"macro\"))\n",
    "print(recall_score(y_test, y_pred, average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cust_metric(y_true, y_pred_proba):\n",
    "    # Keep only one label with the higher proba\n",
    "    row_maxs = y_pred_proba.max(axis=1, keepdims=True)\n",
    "    # Indices of maximum value for each row\n",
    "    maxis = np.where(y_pred_proba == row_maxs, 1, 0)\n",
    "    # 1 if max value is indeed a tag, 0 otherwise\n",
    "    check = y_true[maxis == 1]\n",
    "    return np.mean(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35365853658536583\n"
     ]
    }
   ],
   "source": [
    "print(cust_metric(y_test, y_pred_probas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "########### TRAIN MODEL WITH ALL DATA ###########\n",
    "#################################################\n",
    "\n",
    "pipeline_mlp = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, min_df=.0025, max_df=0.25, ngram_range=(1,3))),\n",
    "            ('svd', TruncatedSVD(algorithm='randomized', n_components=100, random_state=1)),\n",
    "            ('MLP', MLPClassifier(solver='lbfgs',alpha=1e-5,hidden_layer_sizes=(100),random_state=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.25, max_features=None, min_df=0.0025,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf...True, solver='lbfgs', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_mlp.fit(X, Y_mlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_mlp.predict([\"events all the events i love\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mlb.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save pipeline and mlb transformer\n",
    "joblib.dump(pipeline_mlp, \"clf.pkl\")\n",
    "\n",
    "joblib.dump(mlb, \"mlb.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9906542056074766\n",
      "0.9875776397515528\n",
      "0.8772609819121449\n",
      "0.99375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Prediction on train set\n",
    "y_pred = pipeline_mlp.predict(X_test)\n",
    "print(f1_score(y_test, y_pred, average='micro'))\n",
    "print(precision_score(y_test, y_pred, average=\"micro\"))\n",
    "print(precision_score(y_test, y_pred, average=\"macro\"))\n",
    "print(recall_score(y_test, y_pred, average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y_pred[:4])\n",
    "#print(y_test[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN ALL DATA ON SVC (One-vs-Rest and class balanced)\n",
    "pipeline_svc = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, min_df=.0025, max_df=0.25, ngram_range=(1,3))),\n",
    "            ('svd', TruncatedSVD(algorithm='randomized', n_components=100, random_state=1)),\n",
    "            ('SVC', OneVsRestClassifier(SVC(C=10, gamma=1, kernel=\"rbf\", decision_function_shape='ovo',class_weight=\"balanced\", probability=True)))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.25, max_features=None, min_df=0.0025,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf...ility=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          n_jobs=None))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_svc.fit(X, Y_mlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ynew_pred = pipeline_svc.predict(Xnew)\n",
    "ynew_predProba = pipeline_svc.predict_proba(Xnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8664850136239782\n",
      "0.7681159420289855\n",
      "0.718175493756889\n",
      "0.99375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mondeca/applications/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Prediction on train set\n",
    "y_pred = pipeline_svc.predict(X_test)\n",
    "print(f1_score(y_test, y_pred, average='micro'))\n",
    "print(precision_score(y_test, y_pred, average=\"micro\"))\n",
    "print(precision_score(y_test, y_pred, average=\"macro\"))\n",
    "print(recall_score(y_test, y_pred, average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Load new LoV vocabs #\n",
    "#######################\n",
    "\n",
    "with open('newDATA.pkl', 'rb') as handle:\n",
    "    Xnew, vocabs = pickle.load(handle)\n",
    "    \n",
    "Xnew = np.array(Xnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "def getTags(pipeline, Xdata):\n",
    "    ynew_pred = pipeline.predict(Xnew)\n",
    "    ynew_predProba = pipeline.predict_proba(Xnew)\n",
    "    \n",
    "    # Get one tag for each prediction by the highest value in the predicted vector\n",
    "    row_maxs = ynew_predProba.max(axis=1, keepdims=True)\n",
    "    # Indices of maximum value for each row\n",
    "    ynew_pred1 = np.where(ynew_predProba == row_maxs, 1, 0)\n",
    "    \n",
    "    print(vocabs)\n",
    "    newPreds = mlb.inverse_transform(ynew_pred)\n",
    "    newPreds1 = mlb.inverse_transform(ynew_pred1)\n",
    "    \n",
    "    pp = pprint.PrettyPrinter()\n",
    "    pp.pprint([(x,y,z) for x,y,z in zip(vocabs, newPreds, newPreds1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UsabilityOntology.rdf', 'cultural-event.owl', 'munc.owl', 'catalogue.owl', 'terms.ttl', 'denotative-description.owl', 'context-description.owl', 'ontology.ttl', 'location.owl', 'core.owl', 'vir.ttl', 'arco.owl']\n",
      "[('UsabilityOntology.rdf', (), ('API',)),\n",
      " ('cultural-event.owl', ('Events', 'Multimedia'), ('Events',)),\n",
      " ('munc.owl', ('RDF',), ('RDF',)),\n",
      " ('catalogue.owl', ('Catalogs', 'Government'), ('Government',)),\n",
      " ('terms.ttl', ('General & Upper', 'Services'), ('General & Upper',)),\n",
      " ('denotative-description.owl',\n",
      "  ('Catalogs', 'Events', 'Government'),\n",
      "  ('Catalogs',)),\n",
      " ('context-description.owl',\n",
      "  ('Catalogs', 'Events', 'Multimedia'),\n",
      "  ('Catalogs',)),\n",
      " ('ontology.ttl', ('Industry', 'Services'), ('Industry',)),\n",
      " ('location.owl', (), ('Events',)),\n",
      " ('core.owl', ('Events',), ('Events',)),\n",
      " ('vir.ttl', (), ('Support',)),\n",
      " ('arco.owl', ('Catalogs', 'Events', 'Multimedia'), ('Catalogs',))]\n"
     ]
    }
   ],
   "source": [
    "getTags(pipeline_mlp, Xnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UsabilityOntology.rdf', 'cultural-event.owl', 'munc.owl', 'catalogue.owl', 'terms.ttl', 'denotative-description.owl', 'context-description.owl', 'ontology.ttl', 'location.owl', 'core.owl', 'vir.ttl', 'arco.owl']\n",
      "[('UsabilityOntology.rdf', (), ('API',)),\n",
      " ('cultural-event.owl',\n",
      "  ('Catalogs', 'Events', 'Government', 'Multimedia'),\n",
      "  ('Catalogs',)),\n",
      " ('munc.owl', (), ('RDF',)),\n",
      " ('catalogue.owl',\n",
      "  ('Catalogs', 'Events', 'Government', 'Multimedia'),\n",
      "  ('Catalogs',)),\n",
      " ('terms.ttl', ('General & Upper', 'RDF', 'Services'), ('RDF',)),\n",
      " ('denotative-description.owl',\n",
      "  ('Catalogs', 'Environment', 'Events', 'Government', 'Multimedia'),\n",
      "  ('Catalogs',)),\n",
      " ('context-description.owl',\n",
      "  ('Catalogs', 'Environment', 'Events', 'Government', 'Multimedia'),\n",
      "  ('Catalogs',)),\n",
      " ('ontology.ttl', ('Industry', 'Services'), ('Industry',)),\n",
      " ('location.owl',\n",
      "  ('Catalogs', 'Events', 'Government', 'Multimedia'),\n",
      "  ('Catalogs',)),\n",
      " ('core.owl',\n",
      "  ('Catalogs', 'Events', 'Government', 'Multimedia'),\n",
      "  ('Catalogs',)),\n",
      " ('vir.ttl', (), ('Support',)),\n",
      " ('arco.owl',\n",
      "  ('Catalogs', 'Environment', 'Events', 'Government', 'Multimedia'),\n",
      "  ('Catalogs',))]\n"
     ]
    }
   ],
   "source": [
    "getTags(pipeline_svc, Xnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one tag for each prediction by the highest value in the predicted vector\n",
    "row_maxs = ynew_predProba.max(axis=1, keepdims=True)\n",
    "# Indices of maximum value for each row\n",
    "ynew_pred1 = np.where(ynew_predProba == row_maxs, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UsabilityOntology.rdf', 'cultural-event.owl', 'munc.owl', 'catalogue.owl', 'terms.ttl', 'denotative-description.owl', 'context-description.owl', 'ontology.ttl', 'location.owl', 'core.owl', 'vir.ttl', 'arco.owl']\n"
     ]
    }
   ],
   "source": [
    "print(vocabs)\n",
    "newPreds = mlb.inverse_transform(ynew_pred)\n",
    "newPreds1 = mlb.inverse_transform(ynew_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('UsabilityOntology.rdf', (), ('API',)),\n",
       " ('cultural-event.owl', ('Events', 'Multimedia'), ('Events',)),\n",
       " ('munc.owl', ('RDF',), ('RDF',)),\n",
       " ('catalogue.owl', ('Catalogs', 'Government'), ('Government',)),\n",
       " ('terms.ttl', ('General & Upper', 'Services'), ('General & Upper',)),\n",
       " ('denotative-description.owl',\n",
       "  ('Catalogs', 'Events', 'Government'),\n",
       "  ('Catalogs',)),\n",
       " ('context-description.owl',\n",
       "  ('Catalogs', 'Events', 'Multimedia'),\n",
       "  ('Catalogs',)),\n",
       " ('ontology.ttl', ('Industry', 'Services'), ('Industry',)),\n",
       " ('location.owl', (), ('Events',)),\n",
       " ('core.owl', ('Events',), ('Events',)),\n",
       " ('vir.ttl', (), ('Support',)),\n",
       " ('arco.owl', ('Catalogs', 'Events', 'Multimedia'), ('Catalogs',))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x,y,z) for x,y,z in zip(vocabs, newPreds, newPreds1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST RANDOM FOREST\n",
    "pipeline_RF = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, min_df=.0025, max_df=0.25, ngram_range=(1,3))),\n",
    "            ('svd', TruncatedSVD(algorithm='randomized', n_components=100, random_state=1)),\n",
    "            ('RF', RandomForestClassifier(n_estimators=200, random_state=1, n_jobs=50))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.25, max_features=None, min_df=0.0025,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf...timators=200, n_jobs=50,\n",
       "            oob_score=False, random_state=1, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "pipeline_RF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline_RF.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probas = pipeline_RF.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 43)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_probas_proc = np.array(y_pred_probas)\n",
    "y_pred_probas_proc = y_pred_probas_proc[:,:,1].T\n",
    "y_pred_probas_proc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04 , 0.02 , 0.005, ..., 0.03 , 0.025, 0.   ],\n",
       "       [0.15 , 0.01 , 0.005, ..., 0.075, 0.065, 0.   ],\n",
       "       [0.02 , 0.005, 0.035, ..., 0.035, 0.015, 0.005],\n",
       "       ...,\n",
       "       [0.015, 0.01 , 0.015, ..., 0.055, 0.03 , 0.   ],\n",
       "       [0.02 , 0.005, 0.   , ..., 0.02 , 0.   , 0.005],\n",
       "       [0.04 , 0.06 , 0.015, ..., 0.015, 0.01 , 0.01 ]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_probas_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 43)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_probas_proc.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(cust_metric(y_test, y_pred_probas_proc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "############## DOC2VEC approach #################\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "# Split (get explicit labels and no mlb)\n",
    "X_train, X_test, y_train, y_test, vocabs_train, vocabs_test = train_test_split(X, Y, vocabs, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged = [TaggedDocument(words = Tokenizer(doc), tags=tags) for doc, tags in zip(X_train, y_train)]\n",
    "test_tagged = [TaggedDocument(words = Tokenizer(doc), tags=tags) for doc, tags in zip(X_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 524/524 [00:00<00:00, 1213592.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from tqdm import tqdm\n",
    "from sklearn import utils\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Building vocab\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=300, window = 5,  negative=10, hs=0, min_count=2, sample = 0, workers=cores, seed=0)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 1s, sys: 625 ms, total: 3min 2s\n",
      "Wall time: 25.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_dbow.train(train_tagged, total_examples=len(train_tagged), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform text \n",
    "def doc_to_vec(X_tagged):\n",
    "    vect = [model_dbow.infer_vector(x[0]) for x in X_tagged]\n",
    "    return np.array(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect = doc_to_vec(train_tagged)\n",
    "X_test_vect = doc_to_vec(test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(524, 300)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_mlb = mlb.transform(y_train)\n",
    "y_test_mlb = mlb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_train_mlb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(solver='lbfgs',alpha=1e-5,hidden_layer_sizes=(200, 100),random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(200, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train_vect, y_train_mlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_test_vect)\n",
    "y_pred_proba = mlp.predict_proba(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29687499999999994"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test_mlb, y_pred, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3106060606060606"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_metric(y_test_mlb, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=0, n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train_vect, y_train_mlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test_vect)\n",
    "y_pred_proba = rf.predict_proba(X_test_vect)\n",
    "y_pred_probas_proc = np.array(y_pred_probas)\n",
    "y_pred_probas_proc = y_pred_probas_proc[:,:,1].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03592814371257485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f1_score(y_test_mlb, y_pred, average=\"micro\"))\n",
    "cust_metric(y_test_mlb, y_pred_probas_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRID SEARCH FOR SVC ####\n",
    "\n",
    "svc = OneVsRestClassifier(SVC(class_weight=\"balanced\"), n_jobs=-1)\n",
    "            \n",
    "params={'estimator__C':[1, 5, 10],\n",
    "        'estimator__kernel':[\"rbf\", \"sigmoid\", \"linear\"],\n",
    "        'estimator__gamma':['auto', 1, 2],\n",
    "        'estimator__decision_function_shape' : [\"ovo\", \"ovr\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['estimator__C', 'estimator__cache_size', 'estimator__class_weight', 'estimator__coef0', 'estimator__decision_function_shape', 'estimator__degree', 'estimator__gamma', 'estimator__kernel', 'estimator__max_iter', 'estimator__probability', 'estimator__random_state', 'estimator__shrinking', 'estimator__tol', 'estimator__verbose', 'estimator', 'n_jobs'])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=OneVsRestClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False),\n",
       "          n_jobs=-1),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'estimator__C': [1, 5, 10], 'estimator__kernel': ['rbf', 'sigmoid', 'linear'], 'estimator__gamma': ['auto', 1, 2], 'estimator__decision_function_shape': ['ovo', 'ovr']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_micro', verbose=0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(svc, params, cv=3, scoring=\"f1_micro\")\n",
    "gs.fit(X_proc,Y_mlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'estimator__C': 5,\n",
       " 'estimator__decision_function_shape': 'ovo',\n",
       " 'estimator__gamma': 'auto',\n",
       " 'estimator__kernel': 'linear'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
